{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625a5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51558149",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../configs/pcgu_config.yaml\"\n",
    "with open(config_path, \"r\") as yamlfile:\n",
    "    config = yaml.load(yamlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c85d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /ist/users/sitipornl/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Aug  7 19:14:12 2023) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(config[\"validation_metric\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dcff240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias model acc : {'accuracy': 0.4386812392093751}\n"
     ]
    }
   ],
   "source": [
    "biased_label_maps = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "main_label_maps = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "biased_label_remaps = {v:k for k,v in biased_label_maps.items()}\n",
    "main_label_remaps   = {v:k for k,v in main_label_maps.items()}\n",
    "data_path  = config[\"data_path\"]\n",
    "train_data = config[\"train_data\"]\n",
    "data_path = os.path.join('../',data_path, train_data)\n",
    "biased_df = pd.read_json(data_path, lines=True)\n",
    "seed = str(config['seed'])\n",
    "\n",
    "predictions = []\n",
    "results = []\n",
    "\n",
    "# ************* Biased model **************\n",
    "for index, row in biased_df.iterrows():\n",
    "    prediction =  biased_label_remaps[int(torch.argmax(torch.Tensor(row['bias_probs']), dim=0))]\n",
    "    predictions.append(prediction)\n",
    "    # results.append(prediction  == \"entailment\")\n",
    "    results.append(prediction  == row['gold_label'])\n",
    "\n",
    "biased_df['predictions'] = predictions\n",
    "biased_df['results'] = results\n",
    "biased_df['gold_label_ids'] = biased_df['gold_label'].apply(lambda row : biased_label_maps[row])\n",
    "biased_df['prediction_ids'] = biased_df['predictions'].apply(lambda row : biased_label_maps[row])\n",
    "\n",
    "print(f\"Bias model acc : {metric.compute(predictions=biased_df['prediction_ids'].tolist() , references=biased_df['gold_label_ids'].tolist() ) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efceec7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias model performance\n",
      "entailment class : {'accuracy': 0.5876515481401692}\n",
      "contradiction class : {'accuracy': 0.34619527436346}\n",
      "neutral class : {'accuracy': 0.3822001527883881}\n",
      "Over all acc : 0.43868232509733907\n"
     ]
    }
   ],
   "source": [
    "all_acc = 0\n",
    "print(f'Bias model performance')\n",
    "for label_text in biased_label_maps.keys():\n",
    "    mask = biased_df['gold_label'] == label_text\n",
    "    predictions = biased_df['prediction_ids'][mask].tolist()\n",
    "    references  = biased_df['gold_label_ids'][mask].tolist()\n",
    "    cur_acc = metric.compute(predictions=predictions , references=references)\n",
    "    print(f\"{label_text} class : {cur_acc}\")\n",
    "    all_acc += cur_acc['accuracy']\n",
    "print(f'Over all acc : {all_acc / len(biased_label_maps.keys())}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sit_env)",
   "language": "python",
   "name": "sit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
