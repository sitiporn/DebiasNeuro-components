{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def __filter(_sent: List[str]) -> List[str]:\n",
    "    _sent = list(filter(lambda x: re.match(\"\\w+\", x), _sent))\n",
    "    return _sent\n",
    "\n",
    "\n",
    "def vanilla_tokenize(_sent: str, _filter=__filter) -> List[str]:\n",
    "    _words = [x.lower() for x in word_tokenize(_sent)]\n",
    "    _words = _filter(_words)\n",
    "    return _words\n",
    "\n",
    "\n",
    "def lemmatized_tokenize(_sent: str, _lemmatizer=LEMMATIZER, _filter=__filter) -> List[str]:\n",
    "    _words = [_lemmatizer.lemmatize(x.lower(), \"v\")\n",
    "              for x in word_tokenize(_sent)]\n",
    "    _words = _filter(_words)\n",
    "    return _words\n",
    "\n",
    "from typing import List, Callable\n",
    "\n",
    "\n",
    "\n",
    "def get_ngram_doc(\n",
    "    doc: str,\n",
    "    n: int,\n",
    "    tokenize: Callable[[str], List[str]] = vanilla_tokenize\n",
    ")-> List[str]:\n",
    "    tokenized_doc = tokenize(doc)\n",
    "    length = len(tokenized_doc)-(n-1)\n",
    "    return [\n",
    "        '_'.join([tokenized_doc[i+j] for j in range(n)])\n",
    "        for i in range(length)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_ngram_docs(\n",
    "    docs: List[str],\n",
    "    n: int,    \n",
    "    tokenize: Callable[[str], List[str]] = vanilla_tokenize\n",
    ") -> List[List[str]]:\n",
    "    return [\n",
    "        get_ngram_doc(doc=doc, n=n, tokenize=tokenize)\n",
    "        for doc in docs\n",
    "    ]\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "from nltk.sentiment.util import NEGATION_RE\n",
    "\n",
    "\n",
    "def count_negations(\n",
    "    sent: Union[str, List[str]]\n",
    ") -> int:\n",
    "    sent = sent if isinstance(sent, list) else vanilla_tokenize(sent)\n",
    "    negations = list(filter(\n",
    "        lambda x: x is not None,\n",
    "        [NEGATION_RE.search(x) for x in sent]\n",
    "    ))\n",
    "    return len(negations)\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from math import log\n",
    "\n",
    "\n",
    "def lmi(\n",
    "    p_w_l: float,\n",
    "    p_l_given_w: float,\n",
    "    p_l: float\n",
    ") -> float:\n",
    "    return p_w_l * log(p_l_given_w/p_l)\n",
    "\n",
    "\n",
    "def get_ngram_probs(\n",
    "    ngram_docs: List[List[str]],\n",
    "    labels: List[str],\n",
    "    possible_labels: List[str] = None\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "        Example of the output:\n",
    "            {\n",
    "                \"SUPPORTS\": {\n",
    "                    \"does_not\": {\n",
    "                        \"p_w_l\": 0.001,\n",
    "                        \"p_l_given_w\": 0.005,\n",
    "                        \"p_l\": 0.002\n",
    "                    },\n",
    "                    \"get_in\": {\n",
    "                        \"p_w_l\": 0.001,\n",
    "                        \"p_l_given_w\": 0.005,\n",
    "                        \"p_l\": 0.002\n",
    "                    }\n",
    "                },\n",
    "                \"REFUTES\": {\n",
    "                    \"did_not\": {\n",
    "                        \"p_w_l\": 0.001,\n",
    "                        \"p_l_given_w\": 0.005,\n",
    "                        \"p_l\": 0.002\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    \"\"\"\n",
    "    possible_labels = possible_labels if possible_labels else list(set(labels))\n",
    "    counter = {label: {} for label in possible_labels} # count(w, l)\n",
    "    n_appear_labels = {label: 0 for label in possible_labels} # count(l)\n",
    "    n_ngrams = {} # count(w)\n",
    "\n",
    "    for ngram_doc, label in zip(ngram_docs, labels):\n",
    "        for ngram in ngram_doc:\n",
    "            counter[label][ngram] = counter[label].get(ngram, 0) + 1\n",
    "            n_ngrams[ngram] = n_ngrams.get(ngram, 0) + 1\n",
    "            n_appear_labels[label] += 1\n",
    "\n",
    "    total_ngrams = sum([n for _, n in n_appear_labels.items()]) # D\n",
    "    prob = {label: {} for label in possible_labels}\n",
    "\n",
    "    for label in possible_labels:\n",
    "        p_l = n_appear_labels[label] / total_ngrams\n",
    "        for ngram in counter[label].keys():\n",
    "            prob[label][ngram] = {\n",
    "                \"p_w_l\": counter[label][ngram] / total_ngrams,\n",
    "                \"p_l_given_w\": counter[label][ngram] / n_ngrams[ngram],\n",
    "                \"p_l\": p_l\n",
    "            }\n",
    "\n",
    "    return prob\n",
    "\n",
    "\n",
    "def compute_lmi(\n",
    "    ngram_docs: List[List[str]],\n",
    "    labels: List[str],\n",
    "    possible_labels: List[str] = None\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "        Example of the output:\n",
    "            {\n",
    "                \"SUPPORTS\": {\n",
    "                    \"does_not\": 0.2,\n",
    "                    \"get_in\": 0.1\n",
    "                },\n",
    "                \"REFUTES\": {\n",
    "                    \"did_not\": 0.2\n",
    "                }\n",
    "            }\n",
    "    \"\"\"\n",
    "    possible_labels = possible_labels if possible_labels else list(set(labels))\n",
    "    ngram_probs = get_ngram_probs(\n",
    "        ngram_docs=ngram_docs,\n",
    "        labels=labels,\n",
    "        possible_labels=possible_labels\n",
    "    )\n",
    "    for label in possible_labels:\n",
    "        for ngrams in ngram_probs[label].keys():\n",
    "            ngram_probs[label][ngrams] = lmi(**ngram_probs[label][ngrams])\n",
    "    return ngram_probs\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "SPACY_NLP = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_lexical_overlap(\n",
    "    sent1: Union[str, List[str]],\n",
    "    sent2: Union[str, List[str]]\n",
    ") -> float:\n",
    "    sent1 = sent1 if isinstance(sent1, list) else vanilla_tokenize(sent1)\n",
    "    sent2 = sent2 if isinstance(sent2, list) else vanilla_tokenize(sent2)\n",
    "\n",
    "    count = 0\n",
    "    for w1 in sent1:\n",
    "        for w2 in sent2:\n",
    "            if w1 == w2:\n",
    "                count += 1\n",
    "\n",
    "    return count / max(len(sent1), len(sent2))\n",
    "\n",
    "\n",
    "def get_entities_overlap(\n",
    "    sent1: str,\n",
    "    sent2: str\n",
    ") -> int:\n",
    "    doc1 = SPACY_NLP(sent1)\n",
    "    doc2 = SPACY_NLP(sent2)\n",
    "\n",
    "    count = 0\n",
    "    for ent1 in doc1.ents:\n",
    "        for ent2 in doc2.ents:\n",
    "            if (ent1.text, ent1.label_) == (ent2.text, ent2.label_):\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "def inference_prob_to_index(x: List[Dict[str, float]]) -> List[float]:\n",
    "    return [\n",
    "        x[\"SUPPORTS\"],\n",
    "        x[\"NOT ENOUGH INFO\"],\n",
    "        x[\"REFUTES\"]\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import pickle\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "from sklearn.base import RegressorMixin, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from my_package.models.traditional import TraditionalML, FEATURE_EXTRACTOR\n",
    "# from my_package.utils.handcrafted_features.mutual_information import compute_lmi\n",
    "# from my_package.utils.ngrams import get_ngram_doc, get_ngram_docs\n",
    "# from my_package.utils.tokenizer import vanilla_tokenize\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "\n",
    "FEATURE_EXTRACTOR = Callable[[str, str], float]\n",
    "\n",
    "\n",
    "class TraditionalML(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, docs: List[Tuple[str, str]], labels: List[str]) -> None:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, docs: List[Tuple[str, str]]) -> List[dict]:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, docs: List[Tuple[str, str]]) -> List[str]:\n",
    "        ...\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    \"n_grams\": [1, 2],\n",
    "    \"top_ks\": [50, 50],\n",
    "}\n",
    "\n",
    "DEFAULT_MODEL = LogisticRegression(\n",
    "    random_state=42,\n",
    "    solver='saga',\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "\n",
    "class Classifier(TraditionalML):\n",
    "    def __init__(\n",
    "        self,\n",
    "        possible_labels: List[str],\n",
    "        feature_extractors: List[FEATURE_EXTRACTOR],\n",
    "        tokenizer: Callable[[str], List[str]] = vanilla_tokenize,\n",
    "        normalizer: TransformerMixin = MinMaxScaler(),\n",
    "        model: RegressorMixin = DEFAULT_MODEL,\n",
    "        config: dict = DEFAULT_CONFIG\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "            Currently, tokenizer and normalizer can not be saved to the file.\n",
    "            We have to inject it mannually.\n",
    "        \"\"\"\n",
    "        self.map_labels = {lb: i for i, lb in enumerate(possible_labels)}\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.normalizer = normalizer\n",
    "        self.feature_extractors = feature_extractors\n",
    "\n",
    "        self.config = config\n",
    "        self._validate_config()\n",
    "        # internal states\n",
    "        self.top_ngrams_sent1 = None\n",
    "        self.top_ngrams_sent2 = None\n",
    "        self.words_to_idx = None\n",
    "        self.n_features = None\n",
    "\n",
    "    def save(self, folder: str) -> None:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        _model = {\n",
    "            \"map_labels\": self.map_labels,\n",
    "            \"model\": self.model,\n",
    "            \"normalizer\": self.normalizer,\n",
    "        }\n",
    "        _states = {\n",
    "            \"config\": self.config,\n",
    "            \"top_ngrams_sent1\": self.top_ngrams_sent1,\n",
    "            \"top_ngrams_sent2\": self.top_ngrams_sent2,\n",
    "            \"words_to_idx\": self.words_to_idx,\n",
    "            \"n_features\": self.n_features\n",
    "        }\n",
    "        pickle.dump(_model, open(os.path.join(folder, \"model.pickle\"), 'wb'))\n",
    "        pickle.dump(_states, open(os.path.join(folder, \"state.pickle\"), 'wb'))\n",
    "\n",
    "    def load(self, folder: str) -> None:\n",
    "        _model = pickle.load(open(os.path.join(folder, \"model.pickle\"), 'rb'))\n",
    "        self.map_labels = _model[\"map_labels\"]\n",
    "        self.model = _model[\"model\"]\n",
    "        self.normalizer = _model[\"normalizer\"]\n",
    "        _states = pickle.load(open(os.path.join(folder, \"state.pickle\"), 'rb'))\n",
    "        self.config = _states[\"config\"]\n",
    "        self.top_ngrams_sent1 = _states[\"top_ngrams_sent1\"]\n",
    "        self.top_ngrams_sent2 = _states[\"top_ngrams_sent2\"]\n",
    "        self.words_to_idx = _states[\"words_to_idx\"]\n",
    "        self.n_features = _states[\"n_features\"]\n",
    "\n",
    "    def _validate_config(self):\n",
    "        assert len(self.config.get(\"n_grams\")) \\\n",
    "            == len(self.config.get(\"top_ks\"))\n",
    "\n",
    "    def _get_top_n_grams(self, docs: List[str], labels: List[str]) -> Dict[int, List[str]]:\n",
    "        top_ngrams = {}\n",
    "        for n, top_k in zip(self.config.get(\"n_grams\"), self.config.get(\"top_ks\")):\n",
    "            ngram_docs = get_ngram_docs(\n",
    "                docs=docs, n=n,\n",
    "                tokenize=self.tokenizer\n",
    "            )\n",
    "            lmis = compute_lmi(ngram_docs=ngram_docs, labels=labels)\n",
    "            top_k_lmis = {\n",
    "                label: dict(sorted(\n",
    "                    lmi.items(), key=operator.itemgetter(1), reverse=True\n",
    "                )[:top_k])\n",
    "                for label, lmi in lmis.items()\n",
    "            }\n",
    "            if self.config.get(\"verbose\", False):\n",
    "                print(\"%d-gram LMI: \" % n, top_k_lmis, \"\\n\")\n",
    "            top_ngrams[n] = []\n",
    "            for _, lmi in top_k_lmis.items():\n",
    "                top_ngrams[n].extend(lmi.keys())\n",
    "        return top_ngrams\n",
    "\n",
    "    def _transform(self, doc: Tuple[str, str]) -> List[float]:\n",
    "        n_tokens = len(self.map_labels) * sum(self.config.get(\"top_ks\"))\n",
    "        vec_output = [0, ] * self.n_features\n",
    "\n",
    "        for n in self.config.get(\"n_grams\"):\n",
    "            ngram_sent1 = get_ngram_doc(\n",
    "                doc[0], n=n,\n",
    "                tokenize=self.tokenizer\n",
    "            )\n",
    "            for i, token in enumerate(ngram_sent1):\n",
    "                if token in self.words_to_idx:\n",
    "                    idx = self.words_to_idx[token]\n",
    "                    vec_output[idx] += 1\n",
    "\n",
    "            ngram_sent2 = get_ngram_doc(\n",
    "                doc[1], n=n,\n",
    "                tokenize=self.tokenizer\n",
    "            )\n",
    "            for i, token in enumerate(ngram_sent2):\n",
    "                if token in self.words_to_idx:\n",
    "                    idx = self.words_to_idx[token]\n",
    "                    vec_output[idx] += 1\n",
    "\n",
    "        for i, f in enumerate(self.feature_extractors):\n",
    "            idx = 2 * n_tokens + i\n",
    "            vec_output[idx] = f(doc[0], doc[1])\n",
    "        return vec_output\n",
    "\n",
    "    def fit(self, docs: List[Tuple[str, str]], labels: List[str]) -> None:\n",
    "        sent1s = [d[0] for d in docs]\n",
    "        sent2s = [d[1] for d in docs]\n",
    "        if self.config.get(\"verbose\", False):\n",
    "            print(\"------ Top N-grams for sentence 1 ------\")\n",
    "        self.top_ngrams_sent1 = self._get_top_n_grams(sent1s, labels)\n",
    "        if self.config.get(\"verbose\", False):\n",
    "            print(\"------ Top N-grams for sentence 2 ------\")\n",
    "        self.top_ngrams_sent2 = self._get_top_n_grams(sent2s, labels)\n",
    "\n",
    "        self.words_to_idx = {}\n",
    "        for top_ngrams_sent_i in (self.top_ngrams_sent1,  self.top_ngrams_sent2):\n",
    "            for n in top_ngrams_sent_i:\n",
    "                for w in top_ngrams_sent_i[n]:\n",
    "                    self.words_to_idx[w] = len(self.words_to_idx)\n",
    "\n",
    "        self.n_features = 2 * \\\n",
    "            len(self.map_labels) * sum(self.config.get(\"top_ks\")) + \\\n",
    "            len(self.feature_extractors)\n",
    "        if self.config.get(\"verbose\", False):\n",
    "            print(\"n_features: %d\" % self.n_features)\n",
    "\n",
    "        x = [self._transform(d) for d in docs]\n",
    "        y = [self.map_labels[lb] for lb in labels]\n",
    "\n",
    "        x = self.normalizer.fit_transform(x)\n",
    "        self.model.fit(x, y)\n",
    "\n",
    "    def inference(self, docs: List[Tuple[str, str]]) -> List[dict]:\n",
    "        x = [self._transform(doc) for doc in docs]\n",
    "        x = self.normalizer.transform(x)\n",
    "        y_preds = self.model.predict_proba(x)\n",
    "\n",
    "        possible_labels = self.map_labels.keys()\n",
    "        return [dict(zip(possible_labels, y_pred)) for y_pred in y_preds]\n",
    "\n",
    "    def predict(self, docs: List[Tuple[str, str]]) -> List[str]:\n",
    "        inverse_mapper = {v: k for k, v in self.map_labels.items()}\n",
    "        x = [self._transform(doc) for doc in docs]\n",
    "        x = self.normalizer.transform(x)\n",
    "        y_preds = self.model.predict(x)\n",
    "        return [inverse_mapper[y] for y in y_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples:  232911\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from random import random\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "\n",
    "# Config\n",
    "DUMMY_PREFIX = \"\" # \"sample_\" for example and \"\" for the real one\n",
    "\n",
    "TRAIN_DATA_FILE = \"../../data/fact_verification/%sfever.train.jsonl\"%DUMMY_PREFIX\n",
    "VAL_DATA_FILE = \"../../data/fact_verification/%sfever.val.jsonl\"%DUMMY_PREFIX\n",
    "DEV_DATA_FILE = \"../../data/fact_verification/%sfever.dev.jsonl\"%DUMMY_PREFIX\n",
    "TEST_DATA_FILE = \"../../data/fact_verification/fever_symmetric_v0.1.test.jsonl\"\n",
    "\n",
    "WEIGHT_KEY = \"sample_weight\"\n",
    "OUTPUT_VAL_DATA_FILE = \"../../data/fact_verification/%sweighted_feverv2.val.jsonl\"%DUMMY_PREFIX\n",
    "OUTPUT_TRAIN_DATA_FILE = \"../..//data/fact_verification/%sweighted_feverv2.train.jsonl\"%DUMMY_PREFIX\n",
    "SAVED_MODEL_PATH = \"../results/fever/bias_model\"\n",
    "\n",
    "DOC1_KEY = \"claim\"\n",
    "DOC2_KEY = \"evidence\"\n",
    "LABEL_KEY = \"gold_label\"\n",
    "\n",
    "POSSIBLE_LABELS = (\"SUPPORTS\", \"NOT ENOUGH INFO\", \"REFUTES\")\n",
    "BIAS_CLASS = \"REFUTES\"\n",
    "\n",
    "MAX_SAMPLE = -1 # -1 for non-maximal mode or a finite number e.g. 2000\n",
    "DROP_RATE = 0.0\n",
    "TEST_FRAC = 0.2\n",
    "\n",
    "MAX_TEST_SAMPLE = -1\n",
    "\n",
    "def read_data(\n",
    "    file: str = TRAIN_DATA_FILE,\n",
    "    sent1_key: str = DOC1_KEY,\n",
    "    sent2_key: str = DOC2_KEY,\n",
    "    label_key: str = LABEL_KEY,\n",
    "    drop_rate: float = 0.0\n",
    "):\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    N_SAMPLE = 0\n",
    "\n",
    "    with open(file, 'r') as fh:\n",
    "        line = fh.readline()\n",
    "        while line:\n",
    "            if random() > drop_rate:\n",
    "                datapoint = json.loads(line)\n",
    "                docs.append([datapoint[sent1_key], datapoint[sent2_key]])\n",
    "                labels.append(datapoint[label_key])\n",
    "\n",
    "                N_SAMPLE += 1\n",
    "                if MAX_SAMPLE != -1 and N_SAMPLE == MAX_SAMPLE:\n",
    "                    break\n",
    "            line = fh.readline()\n",
    "    print(\"# samples: \", N_SAMPLE)\n",
    "    return docs, labels\n",
    "\n",
    "\n",
    "docs, labels = read_data(drop_rate=DROP_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ca7abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Danny DeVito was nominated for President of the United States .',\n",
       "  'Daniel Michael DeVito , Jr. -LRB- born November 17 , 1944 -RRB- is an American actor , comedian , director and producer .'],\n",
       " ['Star Wars : Episode II â€“ Attack of the Clones came out on Blu-ray disc .',\n",
       "  'The film was released on VHS and DVD on November 12 , 2002 and was later released on Blu-ray on September 16 , 2011 .']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145a136",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ac2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, labels_train, labels_test = train_test_split(\n",
    "    docs, labels,\n",
    "    stratify=labels, test_size=TEST_FRAC,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e2a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractors = [\n",
    "    lambda s1, s2: count_negations(s1),\n",
    "    lambda s1, s2: count_negations(s2),\n",
    "    # get_lexical_overlap,\n",
    "    # get_entities_overlap\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b0190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_grams\": [1, 2],\n",
    "    \"top_ks\": [50, 50], # select by LMI\n",
    "    \"verbose\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e81b32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(\n",
    "    possible_labels=POSSIBLE_LABELS,\n",
    "    feature_extractors=feature_extractors,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb11b171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Top N-grams for sentence 1 ------\n",
      "1-gram LMI:  {'REFUTES': {'not': 0.004404081229555553, 'only': 0.003491047080548929, 'is': 0.002216540970319341, 'to': 0.0010337398643268142, 'incapable': 0.0009450546335391023, 'did': 0.000748672774984025, 'ever': 0.000688767450525235, 'yet': 0.0005503035739134572, 'zero': 0.0004917502990755112, 'any': 0.00044249903782089453, 'exclusively': 0.0004329570953196126, 'never': 0.0004292220736421998, 'being': 0.00042106507095885504, 'refused': 0.00041258899573103536, 'failed': 0.0004046145948679455, 'does': 0.00039396136270673977, \"n't\": 0.0003837027798177204, 'be': 0.00035923270709176754, 'born': 0.00034103916900006954, 'book': 0.0003309228252019722, 'no': 0.0003268041685797059, 'died': 0.0003147610588722846, 'anything': 0.00028018811160279033, 'avoided': 0.000273703960640846, 'refuses': 0.0002602962230195555, 'acting': 0.00023709080290993198, 'unable': 0.00023372655335836986, 'always': 0.00022208696275075168, 'have': 0.0001853776657269754, 'has': 0.000181136657194411, 'life': 0.0001801654914587863, 'entire': 0.00017235556103349309, 'except': 0.00015887407547333088, 'car': 0.0001432490186357342, 'outside': 0.00012665646812372802, 'experience': 0.00012612271966113103, 'act': 0.00012395114006311258, 'been': 0.00011963950690648616, 'squid': 0.00011667738572910516, 'novel': 0.00011337039092227373, 'show': 0.00011198219199160808, 'release': 0.00011041081497346794, 'play': 0.00010854456684368299, 'entirely': 0.00010792720798280361, 'whole': 0.00010773913732935796, 'solely': 0.00010705479156912963, 'win': 0.00010454557225023045, 'before': 0.00010379841316982118, 'single': 0.00010157526453969918, 'appear': 0.00010102711613139432}, 'NOT ENOUGH INFO': {'the': 0.0013313873659237677, 'in': 0.0010729566485549898, 'by': 0.0007590231109370083, 'with': 0.0006799134792325078, 'and': 0.000666099784652125, 'for': 0.0005426393928298476, 'stars': 0.0005337411798748857, 'of': 0.00045338993416841196, 'worked': 0.00026842908666122934, 'starred': 0.00023111369702478264, 'from': 0.00020205967788751092, 'won': 0.00019854515513715624, 'produced': 0.0001957159751894449, 'directed': 0.00019471880954134293, 'on': 0.00019266538916340122, 'reviewed': 0.00016104634187493854, 'appeared': 0.00015229932621194466, 'he': 0.00013386580708825907, 'catholic': 0.00013265850044582518, 'college': 0.00013258774144296254, 'written': 0.00013188559050072006, 'canadian': 0.00013035426183645227, 'two': 0.00012405951488923323, 'wrote': 0.00011590679287011856, 'singer': 0.00010630605491535209, 'attended': 0.00010229217294547752, 'created': 0.00010182495121624117, 'tim': 0.00010015974392005957, 'american': 9.693156555002468e-05, 'about': 9.648528067925208e-05, 'has': 9.34220837165306e-05, 'three': 9.193607796412937e-05, 'school': 9.121377551444484e-05, 'successful': 8.940665979231901e-05, 'rice': 8.888279574162019e-05, 'father': 8.646939377336881e-05, 'new': 8.491545684695367e-05, 'france': 8.301194696810163e-05, 'his': 8.073622522158485e-05, 'had': 7.950592361249373e-05, 'member': 7.868032527073485e-05, 'earp': 7.818372104833717e-05, 'war': 7.817753056213293e-05, 'lost': 7.805921190436248e-05, 'married': 7.804401898967813e-05, 'years': 7.691861304064205e-05, 'most': 7.60437234592292e-05, 'at': 7.549409468399856e-05, 'dancer': 7.530557006977657e-05, 'award-winning': 7.441137642824348e-05}, 'SUPPORTS': {'is': 0.0030679707175702035, 'a': 0.002647572516195074, 'person': 0.0019978349317691184, 'an': 0.0011824944128438549, 'there': 0.0006831033942117711, 'film': 0.0006183735414502595, 'acts': 0.0005621521658285568, 'movie': 0.0004576157010842087, 'actor': 0.0003823907867407434, 'was': 0.0003778306088296258, 'performer': 0.0003537518405108853, 'place': 0.00029313095103915596, 'actress': 0.00028801506799923566, 'album': 0.000262638021229089, 'who': 0.00025387959655144874, 'work': 0.00024149020414800573, 'award': 0.00022350022236634196, 'are': 0.0002109449871517374, 'one': 0.00019949351938014273, 'called': 0.0001871005329896301, 'athlete': 0.0001866082307295716, 'acted': 0.00016676749226226812, 'artist': 0.00016621184275569182, 'plays': 0.00016150054483824884, 'entertainer': 0.00015649015450060186, 'character': 0.0001465902764970688, 'music': 0.0001451559166739474, 'released': 0.0001402969766888249, 'works': 0.00013819140213045005, 'musician': 0.0001381556189658691, 'had': 0.00013440471733266405, 'least': 0.00013174417729654254, 'something': 0.00012531230697443937, 'band': 0.00011871179408743014, 'states': 0.0001136905242433413, 'films': 0.00011187975035145745, 'united': 0.0001103354159925933, 'industry': 0.00010887108177084676, 'group': 0.00010834009225878641, 'made': 0.00010594100795657962, 'country': 0.00010442461325101033, 'williams': 9.916872175363418e-05, 'series': 9.776688776294168e-05, 'professionally': 9.476534564090459e-05, 'as': 9.274563711341422e-05, 'were': 9.172309596518597e-05, 'won': 8.800423730044052e-05, 'winner': 8.247791571083896e-05, 'woman': 8.123007172236369e-05, 'played': 8.079842113417157e-05}} \n",
      "\n",
      "2-gram LMI:  {'REFUTES': {'is_not': 0.002640449737224326, 'not_a': 0.0016235756832440951, 'is_only': 0.00111438978570079, 'incapable_of': 0.001087429932734893, 'only_a': 0.000969455910177536, 'was_not': 0.0008831540922685322, 'did_not': 0.0008759308167324119, 'not_an': 0.0008464382802806952, 'is_incapable': 0.0008066039201877936, 'has_only': 0.0006524899109964303, 'yet_to': 0.0006367568328517187, 'has_yet': 0.000575193318093519, 'of_being': 0.0004949501412108878, 'failed_to': 0.0004693360813139772, 'was_only': 0.00046561475890737503, 'refused_to': 0.00045391964653663613, 'to_be': 0.00042191610525264014, 'only_ever': 0.00042043045509395624, 'does_not': 0.0004116520435242962, 'was_born': 0.000347794469695247, 'a_book': 0.0003010866188973798, 'refuses_to': 0.00029950067181777287, 'born_in': 0.00028921016910835154, 'to_ever': 0.00028351283481609584, 'unable_to': 0.0002703164784511459, 'is_exclusively': 0.0002660529585963023, 'has_no': 0.000256439593076569, 'being_a': 0.00025274546067650293, 'has_avoided': 0.00022691079187265968, 'ever_been': 0.00021991361733140658, 'was_incapable': 0.0002172016818270354, 'died_in': 0.00021391966658075148, 'not_have': 0.00021191282292224695, 'not_in': 0.0002061157473996128, \"is_n't\": 0.00020163592596758936, 'exclusively_a': 0.00019668306670993037, 'a_car': 0.00019412379719655149, 'has_always': 0.00019315893500422015, 'be_an': 0.0001882082741847393, 'has_never': 0.00018568502733532326, 'has_not': 0.00018359963576613302, 'been_a': 0.000173773039462242, 'only_one': 0.00017085824922045726, 'was_unable': 0.00016847823408053174, 'is_a': 0.00016834137305769966, 'television_show': 0.0001664091663731013, 'entire_life': 0.0001613087993229714, 'only_in': 0.00016001108639012338, 'always_been': 0.00014220680098412573, 'a_squid': 0.00014011819257576518}, 'NOT ENOUGH INFO': {'in_a': 0.0004781938644538368, 'of_the': 0.0003186182641395693, 'by_a': 0.0003145102808387272, 'worked_with': 0.00029884753055448157, 'was_in': 0.00029694875605326595, 'stars_a': 0.0002056089801982439, 'won_a': 0.00018134311214720465, 'with_a': 0.00015603424201111787, 'in_an': 0.00014687170724610852, 'has_a': 0.00014616474128034955, 'starred_in': 0.0001416839943605865, 'stars_an': 0.00013827249367486178, 'on_a': 0.00013693159310477994, 'by_an': 0.00013197758227782595, 'appeared_in': 0.0001303865948105973, 'is_the': 0.00012512508135442357, 'and_he': 0.00012029433557288892, 'for_the': 0.00011872854074816346, 'and_it': 0.00011763002175091579, 'he_was': 0.00011692923101100777, 'on_the': 0.00011641362086759511, 'produced_by': 0.00011067338082282361, 'film_directed': 0.00010897063998732179, 'by_the': 0.00010341533540685348, 'was_named': 0.00010219532088619803, 'had_a': 0.00010057525892513435, 'has_been': 0.00010048945701139655, 'won_the': 0.00010028413246263207, 'tim_rice': 9.744177880647663e-05, 'was_produced': 8.918988980687554e-05, 'wyatt_earp': 8.703585376589202e-05, 'a_canadian': 8.660128107713349e-05, 'it_is': 8.497171409909719e-05, 'was_the': 8.434820425859733e-05, 'college_in': 8.353831645946627e-05, 'directed_by': 8.007769985999268e-05, 'for_best': 7.864333903902003e-05, 'a_catholic': 7.699263415935438e-05, 'gift_of': 7.382988745844081e-05, 'the_highest': 7.22124274947999e-05, 'has_worked': 7.168992495853198e-05, 'night_fury': 7.147260747096404e-05, 'of_an': 7.091008253429453e-05, 'written_by': 6.979298204949293e-05, 'reviewed_by': 6.944961641594068e-05, 'actor_born': 6.936318542348574e-05, 'will_ferrell': 6.916292316662017e-05, 'in_france': 6.831070420769367e-05, 'filmed_in': 6.813513566869195e-05, 'in_twilight': 6.792750232718823e-05}, 'SUPPORTS': {'is_a': 0.00398805242922445, 'a_person': 0.002391040450180916, 'is_an': 0.0016050741506241934, 'a_film': 0.0009912305248361568, 'was_a': 0.0007484953419421141, 'there_is': 0.0005838478200560707, 'a_movie': 0.0005142878406403272, 'an_actor': 0.0004697386551836674, 'a_performer': 0.0004278094538333953, 'a_place': 0.0003653507549048446, 'an_album': 0.00033672676978892626, 'person_who': 0.00032640076643154905, 'an_actress': 0.0003177520673416206, 'in_the': 0.0002795145695636325, 'who_acts': 0.0002735193917278823, 'an_award': 0.00026302665919854435, 'was_an': 0.0002626255073744774, 'a_work': 0.00024958600611987787, 'an_athlete': 0.0002285226658343133, 'was_born': 0.0002110976075618669, 'least_one': 0.00020582898213338286, 'a_musician': 0.00019646897139122895, 'a_band': 0.00019402987571288572, 'an_entertainer': 0.00018244117807214382, 'a_character': 0.0001807258608106051, 'at_least': 0.00017702229241484176, 'was_in': 0.00017080315029875366, 'an_artist': 0.00015814174602572954, 'was_released': 0.00015507410433666165, 'in_films': 0.00014250634474316423, 'a_series': 0.0001344525240425187, 'in_a': 0.00013266218262015893, 'a_career': 0.0001247723966866843, 'a_song': 0.00012438271502619204, 'united_states': 0.00012428793838512148, 'won_an': 0.00012185852937428128, 'a_show': 0.00012076564743247041, 'has_an': 0.0001189699317840968, 'a_city': 0.00011241422202366142, 'an_american': 0.00010878699463517461, 'there_are': 0.00010811370404677216, 'the_united': 0.00010521120814502551, 'works_in': 0.00010470612112399158, 'a_country': 0.00010053433884412413, 'a_group': 9.886171548025352e-05, 'stars_in': 9.63476087163357e-05, 'a_motion': 9.539856258498428e-05, 'a_writer': 9.306216800728649e-05, 'motion_picture': 9.107043075743204e-05, 'film_called': 8.996935067939117e-05}} \n",
      "\n",
      "------ Top N-grams for sentence 2 ------\n",
      "1-gram LMI:  {'REFUTES': {'by': 0.00019425839635990559, 'is': 0.00012688276474024878, 'on': 0.00011746249970424777, 'american': 0.00011518903267017224, 'was': 0.0001129656229222099, 'film': 8.918492172006712e-05, 'directed': 7.376802995443734e-05, 'an': 6.76020783920922e-05, 'million': 6.538942811134045e-05, 'born': 6.496135797714225e-05, 'a': 6.129617517658009e-05, 'it': 5.722022973822542e-05, 'premiered': 5.5024576269099964e-05, 'and': 4.933620857337586e-05, 'written': 4.82288378649547e-05, 'released': 4.56403469773114e-05, 'states': 3.824493830174249e-05, '2016': 3.736374487303518e-05, 'one': 3.7261025557547975e-05, 'february': 3.6236162513778866e-05, 'united': 3.55793992813185e-05, 'city': 3.5453634894436504e-05, 'march': 3.404990814503962e-05, 'august': 2.8455065695827065e-05, 'worldwide': 2.800629401063178e-05, 'over': 2.7557110452515075e-05, 'september': 2.6397501961385653e-05, 'largest': 2.439949988396136e-05, '10': 2.400656525607241e-05, 'january': 2.3994224792377988e-05, 'april': 2.3226958137567273e-05, 'to': 2.2889531572611457e-05, 'june': 2.275827657447938e-05, 'may': 2.2718060357174443e-05, 'november': 2.2223723034988863e-05, 'produced': 2.178845876745554e-05, 'populous': 2.1443085526502592e-05, 'time': 2.0105614304499608e-05, 'tennis': 1.9480051230720454e-05, 'season': 1.9473703128186593e-05, 'club': 1.9009810433499973e-05, 'international': 1.8540342655893428e-05, 'football': 1.8342033022672983e-05, 'band': 1.7991465480492433e-05, 'professional': 1.7984726967830737e-05, 'area': 1.793076698755492e-05, 'california': 1.7863753249234388e-05, 'pictures': 1.7809017693816676e-05, 'based': 1.7794014025846175e-05, 'seasons': 1.7127744227161774e-05}, 'NOT ENOUGH INFO': {'t': 0.001223556260701848, 'the': 0.00043746656348440325, 'is': 0.0003680823078437899, 'of': 0.0003604107548796123, 'by': 0.00014051175701696646, 'a': 0.00011482589939594068, 'on': 0.00011267325575136973, 'that': 8.777690910430729e-05, 'name': 8.76119265855993e-05, 'song': 6.820086465233351e-05, 'or': 6.573235045162092e-05, 'it': 6.245154447553931e-05, 'season': 5.997312569078514e-05, 'to': 5.987038838530651e-05, 'character': 5.42770282179748e-05, 'single': 5.185974136554642e-05, 'this': 4.974921567170664e-05, 'comics': 4.5717583164210765e-05, 'album': 4.4096546181512906e-05, 'created': 4.3607280414665085e-05, 'republic': 4.359506153842945e-05, 'are': 4.0571146168246535e-05, 'between': 3.7625070885528736e-05, 'series': 3.753350454326613e-05, 'book': 3.714754319709604e-05, 'language': 3.6399713460052005e-05, 'state': 3.6386491959282434e-05, 'with': 3.6282203202313966e-05, 'based': 3.4287355773951415e-05, 'published': 3.219534086001463e-05, 'novel': 3.1641193506407354e-05, 'same': 3.1004105672219656e-05, 'century': 3.054008814166758e-05, 'queen': 2.9433422653644008e-05, 'county': 2.853092938035524e-05, 'officially': 2.841894974625246e-05, 'marvel': 2.833817229113635e-05, 'concluded': 2.7954885957655275e-05, 'part': 2.783379297879984e-05, 'used': 2.723670009735188e-05, 'who': 2.679537695498363e-05, 'group': 2.648121381973427e-05, 'south': 2.585241881429502e-05, 'originally': 2.5711150822707993e-05, 'from': 2.557506404648252e-05, 'where': 2.5461850536546855e-05, 'comic': 2.5295569724118686e-05, 'korean': 2.4873364069176374e-05, 'company': 2.4711855575343745e-05, 'city': 2.4433939316381216e-05}, 'SUPPORTS': {'for': 0.0004621633564515504, 'and': 0.0003501619965081179, 'in': 0.0003156702792784227, 'award': 0.00026596359488477835, 'best': 0.00025650645906780795, 'as': 0.0001869974080385447, 'she': 0.00018486514144217485, 'he': 0.00017554624546258657, 'her': 0.00016370172451796332, 'awards': 0.00015357863287219985, 'film': 0.00011934568330114024, 'films': 0.0001174476084442494, 'actress': 0.00011611702720499044, 'has': 0.00011280531911573102, 'won': 0.0001109571155131868, 'academy': 0.00010051745648274794, 'golden': 9.017846524583821e-05, 'roles': 8.806607874919713e-05, 'including': 8.757271830699664e-05, 'role': 8.593924427541734e-05, 'actor': 8.139595050294621e-05, 'globe': 7.995182282002801e-05, 'received': 7.650444528096832e-05, 'nominations': 6.010314910450542e-05, 'career': 5.986608640208833e-05, 'starred': 5.808781823200601e-05, 'his': 5.7506483146494195e-05, 'nomination': 5.641402741544873e-05, 'supporting': 5.302488877622245e-05, 'such': 5.212620174093401e-05, 'nominated': 4.9095886758898885e-05, 'performance': 4.5371600360360536e-05, 'which': 4.5257367422496586e-05, 'appeared': 4.2511422475382504e-05, 'born': 3.86123171365725e-05, 'antibodies': 3.8604063793539385e-05, 'comedy': 3.859607354600232e-05, 'time': 3.858316855382145e-05, 'emmy': 3.8114246736638467e-05, '2003': 3.781660920422379e-05, 'three': 3.7752574112402604e-05, 'actors': 3.768967227190852e-05, 'drama': 3.7178749733896584e-05, 'year': 3.656089778004885e-05, '2015': 3.533371790646906e-05, 'champion': 3.5098397788975874e-05, 'world': 3.487021312772487e-05, 'him': 3.442983118791239e-05, '1999': 3.429714780136873e-05, 'acting': 3.256673931842665e-05}} \n",
      "\n",
      "2-gram LMI:  {'REFUTES': {'is_a': 9.063791645502228e-05, 'directed_by': 8.65653412316416e-05, 'film_directed': 5.7363825782401084e-05, 'an_american': 5.639624811384326e-05, 'is_an': 4.488436354454613e-05, 'the_film': 4.0775079657203666e-05, 'the_united': 3.769516267158418e-05, 'written_by': 3.496710960297152e-05, 'is_the': 3.270810227570243e-05, 'united_states': 3.0511491266359664e-05, 'premiered_on': 2.9831480718534355e-05, 'was_an': 2.9176862637287326e-05, 'one_of': 2.8695618309795398e-05, 'and_was': 2.8505761827098776e-05, 'the_series': 2.8139560280687953e-05, 'to_the': 2.731782999008852e-05, 'was_released': 2.704376453432222e-05, 'the_world': 2.655393115406581e-05, 'the_show': 2.575872035797749e-05, 'drama_film': 2.535989343479683e-05, 'film_written': 2.5131289253619226e-05, 'and_directed': 2.4362340726030925e-05, 'and_written': 2.3956832865187904e-05, 'produced_by': 2.3847470885322854e-05, 'it_was': 2.3644609336037654e-05, 'it_is': 2.2939956564237246e-05, 'all_time': 2.0700382965151963e-05, 'of_all': 2.0603640786693586e-05, 'a_2016': 2.0018682996744563e-05, 'based_on': 1.9563653234103554e-05, 'states_on': 1.954790109394284e-05, 'on_march': 1.9000180256307908e-05, 'city_in': 1.88962168598139e-05, 'and_a': 1.8717713029414172e-05, 'released_on': 1.8708847395684568e-05, 'series_of': 1.7313427566055506e-05, 'at_the': 1.7147667917392042e-05, '2016_american': 1.6653146632950273e-05, 'written_and': 1.6263460976963138e-05, 'was_the': 1.5939151699956963e-05, 'on_september': 1.5369948821386546e-05, 'premiered_in': 1.5040424813496538e-05, 'million_worldwide': 1.4784208925269625e-05, 'rock_band': 1.4169887278673974e-05, 'film_was': 1.406445421125985e-05, 'with_a': 1.3913271420639174e-05, 'formed_in': 1.3798329702206348e-05, 'population_of': 1.3328549467844825e-05, 'thriller_film': 1.306636045685022e-05, 'the_only': 1.2499492496703475e-05}, 'NOT ENOUGH INFO': {'t_the': 0.00024258992005250107, 'is_a': 0.0001867370219589455, 't_he': 0.00015541035234607946, 'of_the': 0.00013414880013282927, 't_it': 0.00012413155991186853, 'film_t': 8.419700101787596e-05, 't_in': 6.472152648336727e-05, 'is_the': 6.126693484116907e-05, 't_she': 5.8229701638531036e-05, 'season_of': 5.374053493729585e-05, 'on_the': 4.717500346219161e-05, 'from_the': 4.654693734395923e-05, 'series_t': 4.4262442123883386e-05, 'song_t': 4.347555426390324e-05, 'same_name': 4.3126968226628356e-05, 'by_the': 3.543717311456777e-05, 'the_same': 3.4506261475422794e-05, 'created_by': 3.238612642049257e-05, 'published_by': 3.0065863912310766e-05, 'by_american': 2.931031062392747e-05, 'officially_the': 2.8116481792797847e-05, 't_his': 2.7934519029295295e-05, 't_after': 2.695090920432011e-05, 'a_fictional': 2.6668321602655378e-05, 'of_a': 2.6415485874453156e-05, 'and_concluded': 2.631401753923443e-05, 'concluded_on': 2.6244655101655283e-05, 'the_fifth': 2.5476964727422637e-05, 'marvel_comics': 2.4186289767498792e-05, 'part_of': 2.4049620341662712e-05, 'it_was': 2.335537433761394e-05, 'is_an': 2.3344952195883955e-05, 'republic_of': 2.3123112862423456e-05, 'based_on': 2.2936133529172798e-05, 'album_t': 2.222958204443921e-05, 'books_published': 2.216801124449409e-05, 'tv_series': 2.1413079014971852e-05, 'that_is': 2.1178013526508977e-05, 'comic_books': 2.049747557208042e-05, 'the_sixth': 2.0246633374507612e-05, 't_a': 2.00656404294938e-05, 'television_series': 1.99948901695694e-05, 'american_television': 1.8561108864781814e-05, 'album_by': 1.8434305958836357e-05, 'in_american': 1.7963034692945017e-05, 'song_by': 1.776535634275904e-05, 'a_japanese': 1.7687297319109073e-05, 'of_thrones': 1.7283660516729044e-05, 'a_song': 1.7257493149153953e-05, 'the_name': 1.686059336001722e-05}, 'SUPPORTS': {'for_best': 0.0001720085590725912, 'award_for': 0.00016906778829283797, 'in_the': 0.00011041767852716791, 'golden_globe': 8.600244918130129e-05, 'academy_award': 7.287407837542909e-05, 'for_which': 7.179363368575604e-05, 'and_the': 7.083618890973075e-05, 'best_actress': 6.933284712782406e-05, 'as_a': 6.112844585268348e-05, 'in_a': 5.7622308847621575e-05, 'for_her': 5.705043284178034e-05, 'for_the': 5.547989395588839e-05, 'such_as': 5.203656123572295e-05, 'roles_in': 5.182919179200018e-05, 'which_she': 5.140524457564159e-05, 'best_actor': 4.5525620349400465e-05, 'nominated_for': 4.52411277185514e-05, 'globe_award': 4.4761640227006264e-05, 'starred_in': 4.460951140487911e-05, 'known_for': 4.450054890548662e-05, 'was_nominated': 4.071206965312276e-05, 'for_his': 4.0310050144047e-05, 'best_supporting': 3.9115533742513085e-05, 'appeared_in': 3.73244438873765e-05, 'awards_and': 3.4653432874685615e-05, 'has_also': 3.394241156282246e-05, 'a_golden': 3.393512091596093e-05, 'the_most': 3.309658770225505e-05, 'he_has': 3.293288534308515e-05, 'actress_in': 3.161111972520224e-05, 'won_the': 3.156858348001552e-05, 'which_he': 3.139967163376747e-05, 'all_time': 3.102321359121961e-05, 'academy_awards': 3.098066049816233e-05, 'of_all': 2.9736772973063853e-05, 'role_in': 2.907333035373804e-05, 'nominations_for': 2.864546725271173e-05, 'nomination_for': 2.858709840508005e-05, 'she_has': 2.785667635361388e-05, 'including_the': 2.780145762903099e-05, 'supporting_actress': 2.7447140240235335e-05, 'films_such': 2.65203470848584e-05, 'in_films': 2.6515674712077126e-05, 'she_received': 2.6288969540518192e-05, 'award_nominations': 2.5980553669560393e-05, 'role_as': 2.5845177875079453e-05, 'and_best': 2.5092317592796462e-05, 'as_well': 2.5086300797367767e-05, 'performances_in': 2.3275496245835534e-05, 'well_as': 2.321508421750731e-05}} \n",
      "\n",
      "n_features: 602\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(docs_train, labels_train)\n",
    "classifier.save(SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "869a7e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'SUPPORTS': 0.34859655168976805,\n",
       "  'NOT ENOUGH INFO': 0.5304139330290428,\n",
       "  'REFUTES': 0.12098951528118913},\n",
       " {'SUPPORTS': 0.41778078888520154,\n",
       "  'NOT ENOUGH INFO': 0.4499356385252203,\n",
       "  'REFUTES': 0.1322835725895782}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inferential Examples\n",
    "x = [['Roman Atwood is a content creator .',\n",
    "  'He is best known for his vlogs , where he posts updates about his life on a daily basis .'],\n",
    " ['Roman Atwood is a content creator .',\n",
    "  \"He also has another YouTube channel called `` RomanAtwood '' , where he posts pranks .\"]] \n",
    "classifier.inference(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a798d02",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941ffacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.571\n"
     ]
    }
   ],
   "source": [
    "y_preds = classifier.predict(docs_test)\n",
    "print(\"Accuracy on train set: %.3f\"% accuracy_score(labels_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d011d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.571\n"
     ]
    }
   ],
   "source": [
    "# validate load process of the classifier\n",
    "test_classifier = Classifier(\n",
    "    possible_labels=POSSIBLE_LABELS,\n",
    "    feature_extractors=feature_extractors,\n",
    "    config=config\n",
    ")\n",
    "test_classifier.load(SAVED_MODEL_PATH)\n",
    "\n",
    "y_preds = test_classifier.predict(docs_test)\n",
    "print(\"Accuracy on train set: %.3f\"% accuracy_score(labels_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38118f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples:  16664\n",
      "Accuracy on original test set: 0.412\n",
      "# samples:  717\n",
      "Accuracy on challenge test set: 0.241\n"
     ]
    }
   ],
   "source": [
    "eval_docs, eval_labels = read_data(DEV_DATA_FILE)\n",
    "y_preds = classifier.predict(eval_docs)\n",
    "print(\"Accuracy on original test set: %.3f\"% accuracy_score(eval_labels, y_preds))\n",
    "\n",
    "eval_docs, eval_labels = read_data(TEST_DATA_FILE, sent2_key=\"evidence_sentence\", label_key=\"label\")\n",
    "y_preds = classifier.predict(eval_docs)\n",
    "print(\"Accuracy on challenge test set: %.3f\"% accuracy_score(eval_labels, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99aed8c",
   "metadata": {},
   "source": [
    "# Write predicted probability to the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e19ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(prob_score_ground_truth_class: float) -> float:\n",
    "    return 1/prob_score_ground_truth_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b841009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_weight_to_file(\n",
    "    DATA_FILE: str,\n",
    "    OUTPUT_DATA_FILE: str,\n",
    "    _classifier\n",
    ") -> None:\n",
    "    f_output = open(OUTPUT_DATA_FILE, 'w')\n",
    "\n",
    "    N_SAMPLE = 0\n",
    "\n",
    "    with open(DATA_FILE, 'r') as fh:\n",
    "        line = fh.readline()\n",
    "        while line:\n",
    "            datapoint = json.loads(line)\n",
    "            ground_truth_label = datapoint[LABEL_KEY]\n",
    "            x = [[datapoint[DOC1_KEY], datapoint[DOC2_KEY]]]\n",
    "\n",
    "            probs = _classifier.inference(x)[0]\n",
    "            prob = probs[ground_truth_label]\n",
    "            weight = get_weight(prob_score_ground_truth_class=prob)\n",
    "            if datapoint.get(\"weight\", None) != None:\n",
    "                del datapoint[\"weight\"] # only for fever\n",
    "            f_output.write(\"%s\\n\"%json.dumps({\n",
    "                **datapoint,\n",
    "                WEIGHT_KEY: weight,\n",
    "                \"bias_probs\": inference_prob_to_index(probs),\n",
    "                \"bias_prob\": prob\n",
    "            }))\n",
    "\n",
    "            N_SAMPLE += 1\n",
    "            if MAX_SAMPLE != -1 and N_SAMPLE == MAX_SAMPLE:\n",
    "                break\n",
    "            line = fh.readline()\n",
    "\n",
    "    f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc541849",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_weight_to_file(\n",
    "    DATA_FILE = TRAIN_DATA_FILE,\n",
    "    OUTPUT_DATA_FILE = OUTPUT_TRAIN_DATA_FILE,\n",
    "    _classifier = classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b182383",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_weight_to_file(\n",
    "    DATA_FILE = VAL_DATA_FILE,\n",
    "    OUTPUT_DATA_FILE = OUTPUT_VAL_DATA_FILE,\n",
    "    _classifier = classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5c1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disentangle_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
