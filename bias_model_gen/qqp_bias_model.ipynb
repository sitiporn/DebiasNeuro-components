{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d145a136",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ac2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from debias.datasets.mnli import load_hans, load_mnli, tokenize_examples\n",
    "import py_utils\n",
    "from load_word_vectors import load_word_vectors\n",
    "from tokenizer import NltkAndPunctTokenizer\n",
    "# import tensorflow as tf\n",
    "tok = NltkAndPunctTokenizer()\n",
    "STOP_WORDS = frozenset([\n",
    "  'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
    "  'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n",
    "  'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "  'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',\n",
    "  'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    "  'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by',\n",
    "  'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "  'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "  'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n",
    "  'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "  'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're',\n",
    "  've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma',\n",
    "  'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn',\n",
    "   \"many\", \"how\", \"de\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e2a37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/anaconda3/envs/disentangle_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "# label_maps = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "# reverse_label_maps = {0:\"entailment\", 1:\"contradiction\", 2:\"neutral\"}\n",
    "dataset = {}\n",
    "# One: load data\n",
    "dataset['train'] = pd.read_json('../../data/paraphrase_identification/qqp.train.jsonl', lines=True)\n",
    "dataset['validation'] = pd.read_json('../../data/paraphrase_identification/qqp.val.jsonl', lines=True)\n",
    "dataset['test'] = pd.read_json('../../data/paraphrase_identification/qqp.dev.jsonl', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b0190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']['sentence1']=dataset['train']['sentence1'].apply(tok.tokenize)\n",
    "dataset['train']['sentence2']=dataset['train']['sentence2'].apply(tok.tokenize)\n",
    "dataset['validation']['sentence1']=dataset['validation']['sentence1'].apply(tok.tokenize)\n",
    "dataset['validation']['sentence2']=dataset['validation']['sentence2'].apply(tok.tokenize)\n",
    "dataset['test']['sentence1']=dataset['test']['sentence1'].apply(tok.tokenize)\n",
    "dataset['test']['sentence2']=dataset['test']['sentence2'].apply(tok.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81b32eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221391</td>\n",
       "      <td>169800</td>\n",
       "      <td>76964</td>\n",
       "      <td>[How, can, I, be, a, political, leader, ?]</td>\n",
       "      <td>[How, do, I, become, a, leader, ?]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64746</td>\n",
       "      <td>112460</td>\n",
       "      <td>112461</td>\n",
       "      <td>[How, do, I, check, if, I, have, knock, knees,...</td>\n",
       "      <td>[How, can, I, check, knock, knees, ?]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132701</td>\n",
       "      <td>1993</td>\n",
       "      <td>212438</td>\n",
       "      <td>[What, is, a, good, song, for, lyric, prank, ?]</td>\n",
       "      <td>[What, are, some, good, lyric, pranks, songs, ?]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10227</td>\n",
       "      <td>19843</td>\n",
       "      <td>19844</td>\n",
       "      <td>[What, 's, the, longest, amount, of, time, tha...</td>\n",
       "      <td>[How, long, do, tobacco, /, cannabis, /, alcoh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>392146</td>\n",
       "      <td>156556</td>\n",
       "      <td>224101</td>\n",
       "      <td>[How, long, does, it, take, before, marijuana,...</td>\n",
       "      <td>[How, long, does, it, take, weed, to, get, out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>117628</td>\n",
       "      <td>191369</td>\n",
       "      <td>191370</td>\n",
       "      <td>[How, could, the, universe, appear, out, of, n...</td>\n",
       "      <td>[What, was, there, before, the, Big, Bang, ?, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>293894</td>\n",
       "      <td>237883</td>\n",
       "      <td>415692</td>\n",
       "      <td>[Is, it, better, to, study, at, NITs, or, the,...</td>\n",
       "      <td>[I, have, not, found, reliable, info, online, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>391201</td>\n",
       "      <td>523770</td>\n",
       "      <td>523771</td>\n",
       "      <td>[What, are, some, good, reads, on, Buddhism, ?]</td>\n",
       "      <td>[What, are, the, best, books, on, Buddhism, ?]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>338986</td>\n",
       "      <td>466577</td>\n",
       "      <td>20519</td>\n",
       "      <td>[I, do, n't, like, the, taste, of, water, ., W...</td>\n",
       "      <td>[Does, drinking, lemon, juice, in, the, mornin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>43416</td>\n",
       "      <td>45491</td>\n",
       "      <td>78096</td>\n",
       "      <td>[How, much, do, YouTubers, make, when, each, o...</td>\n",
       "      <td>[How, much, does, a, tech, gadgets, reviewer, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    qid1    qid2  \\\n",
       "0     221391  169800   76964   \n",
       "1      64746  112460  112461   \n",
       "2     132701    1993  212438   \n",
       "3      10227   19843   19844   \n",
       "4     392146  156556  224101   \n",
       "...      ...     ...     ...   \n",
       "4995  117628  191369  191370   \n",
       "4996  293894  237883  415692   \n",
       "4997  391201  523770  523771   \n",
       "4998  338986  466577   20519   \n",
       "4999   43416   45491   78096   \n",
       "\n",
       "                                              sentence1  \\\n",
       "0            [How, can, I, be, a, political, leader, ?]   \n",
       "1     [How, do, I, check, if, I, have, knock, knees,...   \n",
       "2       [What, is, a, good, song, for, lyric, prank, ?]   \n",
       "3     [What, 's, the, longest, amount, of, time, tha...   \n",
       "4     [How, long, does, it, take, before, marijuana,...   \n",
       "...                                                 ...   \n",
       "4995  [How, could, the, universe, appear, out, of, n...   \n",
       "4996  [Is, it, better, to, study, at, NITs, or, the,...   \n",
       "4997    [What, are, some, good, reads, on, Buddhism, ?]   \n",
       "4998  [I, do, n't, like, the, taste, of, water, ., W...   \n",
       "4999  [How, much, do, YouTubers, make, when, each, o...   \n",
       "\n",
       "                                              sentence2  is_duplicate  \n",
       "0                    [How, do, I, become, a, leader, ?]             0  \n",
       "1                 [How, can, I, check, knock, knees, ?]             1  \n",
       "2      [What, are, some, good, lyric, pranks, songs, ?]             1  \n",
       "3     [How, long, do, tobacco, /, cannabis, /, alcoh...             0  \n",
       "4     [How, long, does, it, take, weed, to, get, out...             1  \n",
       "...                                                 ...           ...  \n",
       "4995  [What, was, there, before, the, Big, Bang, ?, ...             1  \n",
       "4996  [I, have, not, found, reliable, info, online, ...             0  \n",
       "4997     [What, are, the, best, books, on, Buddhism, ?]             1  \n",
       "4998  [Does, drinking, lemon, juice, in, the, mornin...             0  \n",
       "4999  [How, much, does, a, tech, gadgets, reviewer, ...             0  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb11b171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "word-vec: 1999996it [00:09, 220473.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# feature preprocessing\n",
    "voc = set()\n",
    "for index, row in dataset['train'].iterrows():\n",
    "    voc.update(row['sentence1'])\n",
    "    voc.update(row['sentence2'])\n",
    "words, vecs = load_word_vectors(\"crawl-300d-2M\", voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869a7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "w2v = {w: v/np.linalg.norm(v) for w, v in zip(words, vecs)}\n",
    "with open(\"w2v_cache\", \"wb\") as f:\n",
    "    pickle.dump(w2v, f)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941ffacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_subseq(needle, haystack):\n",
    "  l = len(needle)\n",
    "  if l > len(haystack):\n",
    "    return False\n",
    "  else:\n",
    "    return any(haystack[i:i+l] == needle for i in range(len(haystack)-l + 1))\n",
    "\n",
    "dataset_to_features = {}\n",
    "for name in dataset.keys():\n",
    "    features = []\n",
    "    for index, row in dataset[name].iterrows():\n",
    "        h = [x.lower() for x in row.sentence1]\n",
    "        p = [x.lower() for x in row.sentence2]\n",
    "        p_words = set(p)\n",
    "        n_words_in_p = sum(x in p_words for x in h)\n",
    "        fe = {\n",
    "        \"h-is-subseq\": is_subseq(h, p),\n",
    "        \"all-in-p\": n_words_in_p == len(h),\n",
    "        \"percent-in-p\": n_words_in_p / len(h),\n",
    "        \"log-len-diff\": np.log(max(len(p) - len(h), 1)),\n",
    "        \"label\": row.is_duplicate\n",
    "        }    \n",
    "        h_vecs = [w2v[w] for w in row.sentence1 if w in w2v]\n",
    "        p_vecs = [w2v[w] for w in row.sentence2 if w in w2v]\n",
    "        if len(h_vecs) > 0 and len(p_vecs) > 0:\n",
    "          h_vecs = np.stack(h_vecs, 0)\n",
    "          p_vecs = np.stack(p_vecs, 0)\n",
    "          # [h_size, p_size]\n",
    "          similarities = np.matmul(h_vecs, p_vecs.T)\n",
    "          # [h_size]\n",
    "          similarities = np.max(similarities, 1)\n",
    "          similarities.sort()\n",
    "          fe[\"average-sim\"] = similarities.sum() / len(h)\n",
    "          fe[\"min-similarity\"] = similarities[0]\n",
    "          if len(similarities) > 1:\n",
    "            fe[\"min2-similarity\"] = similarities[1]\n",
    "\n",
    "        features.append(fe)\n",
    "    dataset_to_features[name] = pd.DataFrame(features)\n",
    "    dataset_to_features[name].fillna(0.0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d011d26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=100, class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100, class_weight=&#x27;balanced&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=100, class_weight='balanced', solver='liblinear')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "train_df = dataset_to_features[\"train\"]\n",
    "feature_cols = [x for x in train_df.columns if x != \"label\"]\n",
    "\n",
    "# class_weight='balanced' will weight the entailemnt/non-entailment examples equally\n",
    "# C=100 means no regularization\n",
    "lr = LogisticRegression(multi_class=\"auto\", solver=\"liblinear\",\n",
    "                    class_weight='balanced', C=100)\n",
    "lr.fit(train_df[feature_cols].values, train_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38118f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train two-class accuracy: 0.6694 (size=394287)\n",
      "validation two-class accuracy: 0.6688 (size=5000)\n",
      "test two-class accuracy: 0.6754 (size=5000)\n"
     ]
    }
   ],
   "source": [
    "for name in dataset_to_features.keys():\n",
    "  examples = dataset_to_features[name]\n",
    "  pred = lr.predict_proba(dataset_to_features[name][feature_cols].values).astype(np.float32)\n",
    "  y = dataset_to_features[name].label.values\n",
    "\n",
    "  acc = np.mean(y == np.argmax(pred, 1))\n",
    "  print(\"%s accuracy: %.4f (size=%d)\" % (name, acc, len(examples)))\n",
    "\n",
    "  dataset[name][\"bias_probs\"]= pred.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One: load data\n",
    "ori_dataset = {}\n",
    "ori_dataset['train'] = pd.read_json('../../data/paraphrase_identification/qqp.train.jsonl', lines=True)\n",
    "ori_dataset['validation'] = pd.read_json('../../data/paraphrase_identification/qqp.val.jsonl', lines=True)\n",
    "ori_dataset['test'] = pd.read_json('../../data/paraphrase_identification/qqp.dev.jsonl', lines=True)\n",
    "for key in dataset.keys():\n",
    "    ori_dataset[key]['bias_probs'] = dataset[key][\"bias_probs\"]\n",
    "    temp_json = ori_dataset[key].to_json(orient='records', lines=True)\n",
    "    with open('qqp_clark.'+key+'.jsonl', 'w') as json_file:\n",
    "        json_file.write(temp_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disentangle_project",
   "language": "python",
   "name": "disentangle_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
