config: ./configs/tiny_masking_rep.yaml
== statistic ==
{'High-overlap': 0.7777777777777778, 'Low-overlap': 0.0}
Counterfactual type: ['High-overlap']
Intervention type : weaken
current model path : ../models/developing_baseline/seed_42/checkpoint-500/pytorch_model.bin
Loading path for single at seed:42, layer: 11
Q: ../counterfactuals/seed_42/avg_Q_counterfactual_representation.pickle
K: ../counterfactuals/seed_42/avg_K_counterfactual_representation.pickle
V: ../counterfactuals/seed_42/avg_V_counterfactual_representation.pickle
AO: ../counterfactuals/seed_42/avg_AO_counterfactual_representation.pickle
I: ../counterfactuals/seed_42/avg_I_counterfactual_representation.pickle
O: ../counterfactuals/seed_42/avg_O_counterfactual_representation.pickle
NIE_paths: ['../NIE/seed_42/avg_embeddings_High-overlap_computed_all_layers_.pickle']
Using original model
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.8487591537835639
contradiction acc : 0.8611111111111112
entailment acc : 0.859370488016171
neutral acc : 0.8242249920102269
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9774
subsequence: 0.9976
constituent: 0.995
Heuristic  non-entailed results:
lexical_overlap: 0.2408
subsequence: 0.055
constituent: 0.1372
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5671666860580444
has score :0.5671666860580444
==================== Avearge scores ===================
average overall acc : 0.8487591537835639
averge contradiction acc : 0.8611111111111112
average entailment acc : 0.859370488016171
average neutral acc : 0.8242249920102269
avarge hans score : 0.5671666860580444
