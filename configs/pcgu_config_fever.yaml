{ 
# ************* experiment *******************
  "collect_representation": False,
  "DEBUG": True,
  "debug": False, 
  "upper_bound": 95,
  "lower_bound": 5,
  "num_samples": 300,
  "label_maps": {"SUPPORTS": 0, "REFUTES": 1, "NOT ENOUGH INFO": 2}, # ours
  "layers": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
  "heads": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
  "collect_param": False,
  "claim_only": False,
  # ******************** PATH ********************
  "range_percents": True, 
  "model_name": "../bert-base-uncased-mnli/",
  "dev_path": "../data/fact_verification/",
  # validation set
  "exp_json": "fever.val.jsonl",
  "dev_json": {},
  # data settings
  "counterfactual_paths": [],
  "NIE_paths": [],
  "is_NIE_exist": [],
  "is_counterfactual_exist": [],
  # ************ Identifying Bias: Causal Mediation Analysis *************
  "eval_candidates": True,
  "treatment": True,
  "is_group_by_class": False,
  "is_averaged_embeddings": True,
  "intervention_type": "weaken",
  "computed_all_layers": True,
  "compare_frozen_weight": True, 
  "print_config": False,
  "top_neuron_mode": "sorted",
  "prunning": True,
  "get_candidate_neurons": True,
  "distribution": False,
  # To get counterfactual
  "compute_all_seeds": False,
  "eval_counterfactual": True,
  "getting_counterfactual": False,
  "compute_nie_scores": False,
  # ******************** soft masking neurons ********************
  "weaken_rate": null,
  "get_masking_value": False,
  # percent of neurons to mask
  "masking_rate": 0.05, 
  "neuron_group": null,
  "rank_losses": False, 
  # range 0.0 to 0.10, step = 0.01
  # searching masking rates
  "percent": {
    "low": 0.0, 
    "high": 0.1,
    "step": 0.01,
    "size": 50,
    "mode": 0o666,
    "acc": {}
    },
  # range 0 to 1, step = 0.1
  "epsilons": {
    "low":  0, 
    "high": 1,
    "step": 0.1, #0.0001, #0.01, 0.1,
    "size": 50,
    "mode": 0o666,
    "acc": {}
    },
  "single_neuron": False,
  # do hyperparameter search on value used modify neurons's activations
  "soft_masking_value_search": True,
  # do hyperparameter search on the percentages of all neurons to mask
  "masking_rate_search": False,
  "get_condition_inferences": False,
  "get_condition_inference_scores": False,
  # ******************** test  stuff ********************
  "eval_model": True,
  "traced": False, 
  "traced_params": False,
  "diag": False,
  # "diag": True,
  "test_traced_params": False,
  "debias_test": False,
  
  # evaluate hans
  "eval": {
    "do": 'High-overlap',
    "layer": 11,
    "debug": False,
    "all_layers": True,
    "intervention_mode": "Intervene"
    },

  # uppper bound test on test set
  # use to read prediction txt files
  "prediction_path": '../pickles/prediction/',
  "evaluations":  {},
  "to_text": True,
  # "get_result": False,
  "get_condition_symm_result": False,
  "get_symm_result": True,
  "save_rank": iment_configTrue,
  # "get_result": False,
  "get_condition_hans_result": False,
  "get_hans_result": True,
  "save_rank": True,
  # neurons components
  "attention_components": 3,
  "nums": {},

# ************ train model  *******************
# ideas from : https://github.com/c4n/debias_nlu/blob/9d3f55449a15fe6e4538ed5713530207d2127afd/configs/nli/baseline/mnli_bert_base_clark_1.jsonnet#L4
  "dataset_reader": {
    "type": "snli",
    "tokenizer": {
      "type": "pretrained_transformer",
      "model_name": "../bert-base-uncased-mnli/",
      "add_special_tokens": false
    }
  },
    
  # Tokenizer
  "tokens": {
    "type": "pretrained_transformer",
    "model_name":  "../bert-base-uncased-mnli/", 
    "max_length": 512
  },
  
  
  # dataset
  "data_path": "../data/fact_verification",
  "train_data": "fever_claim_only.train.jsonl",
  # "train_data": "weighted_feverv2.train.jsonl",
  "validation_data": "fever_claim_only.val.jsonl", 
  # "validation_data": "weighted_feverv2.val.jsonl", 
  "test_data": "fever_claim_only.dev.jsonl",
  
  # dataloader
  "data_loader": {
    "batch_sampler": {
       # bucket type in allenlp by grouping by length and dynamic padding
      "group_by_length": True, 
      "dynamic_padding": True,
      "batch_size" : 32 
    }
  },

  # models; 
  # follow allennlp setups, the rest is the default of bert-base-uncased
  "model": {
    # this model will be replaced with competitive models
    "model_name": "../bert-base-uncased-mnli/",
    transformer_model,
    "max_length": 512,
    "is_load_trained_model": True
  },
  "dropout": 0.1,
  "namespace": "tags",
  
  # Trainer
  "num_epochs": 15,
  # recheck in Trainer 
  "validation_metric": "accuracy",
  "learning_rate_scheduler": {
    "type": "slanted_triangular", # override from allennlp
    "cut_frac": 0.06
  },
  "optimizer": {
    # using optimizer from the transformers package
    "type": "huggingface_adamw", 
    "lr": 2.0e-6 ,  #2.909e-07(ours), #2e-6(PCGU), #5e-5,
    "weight_decay": 0.1,
  },
  "cuda_device" : 0,
  "evaluation_strategy": "steps",
  "eval_steps": 100,
  "load_best_model_at_end": True,
  "save_total_limit": 1,
  # "use_amp" in allennlp
  "half_precision_backend": "amp",
}