{ 
# Mitigating Spurious Correlation in Natural Language Understanding with Counterfactual Inference
# Main Model We apply the debiasing methods on the BERT base model (uncased) (Devlin et al., 2019). 
# ideas from : https://github.com/c4n/debias_nlu/blob/9d3f55449a15fe6e4538ed5713530207d2127afd/configs/nli/baseline/mnli_bert_base_clark_1.jsonnet#L4
# ************* experiment *******************
  "collect_representation": False,
  "DEBUG": True,
  "debug": False, 
  "num_samples": 300,
  "upper_bound": 95,
  "lower_bound": 5,
  "candidated_class": ["entailment"], 
  "intervention_class": ["entailment"], 
  "dataset_name":  'mnli',
  "label_maps": {"entailment": 0, "contradiction": 1, "neutral": 2}, # ours
  # select a neuron mode to train eg. "random" or  "sorted"
  "top_neuron_mode": "sorted",
  # *********  select advantaged samples set up to train  *****
  "random_adv": True,
  "collect_adv": False, 
  "correct_pred": True,
  "layers": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
  "heads": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
  "collect_param": False,
  # ******************** PATH ********************
  "num_top_neurons": null, 
  "range_percents": True, 
  'neurons': True, 
  "model_name": "../bert-base-uncased-mnli/",
  "dev_path": "../debias_fork_clean/debias_nlu_clean/data/nli/",
  "exp_json": "multinli_1.0_dev_matched.jsonl",
  "dev_json": {},
  # data settings
  "counterfactual_paths": [],
  "NIE_paths": [],
  "is_NIE_exist": [],
  "is_counterfactual_exist": [],
  # ************ Identifying Bias: Causal Mediation Analysis *************
  # experiment set 
  # 5, 1
  "k":  5, 
  # rank based on nie scores 
  "eval_candidates": True,
  "layer": 11,
  "treatment": True,
  "is_group_by_class": False,
  "is_averaged_embeddings": True,
  "intervention_type": "replace",
  "computed_all_layers": True,
  # for Custom Adam optmizer
  "compare_frozen_weight": True, 
  "print_config": False,
  "criterion": 'topk',
  # 2, 5, 10, 20, 30, 40, 50, ..,80
  "num_neurons": 10,
  "seeds": [1548, 3099, 3785, 3990, 409], 
  # 1548, 3099, 3785, 3990, 409 for experiments
  "seed": 409, # set to null when using ishan/bert-base-uncased-mnli
  "get_candidate_neurons": True,
  "distribution": False,
  # To get counterfactual
  "compute_all_seeds": True,
  "eval_counterfactual": False,
  "getting_counterfactual": False,
  "compute_nie_scores": False,
  # ******************** Unlearn Bias ********************
  "training_json":  multinli_1.0_train.jsonl, 
  "partition_params": False,
# ************ train model  *******************
  "dataset_reader": {
    "type": "snli",
    "tokenizer": {
      "type": "pretrained_transformer",
      "model_name": "../bert-base-uncased-mnli/",
      "add_special_tokens": false
    }
  },
    
  # Tokenizer
  "tokens": {
    "type": "pretrained_transformer",
    "model_name": "../bert-base-uncased-mnli/", 
    "max_length": 512
  },
  
  # dataset
  "data_path": "../debias_fork_clean/debias_nlu_clean/data/nli/",
  "train_data": "mnli_clark.train.jsonl",
  # "train_data": "train_prob_korn_full_lr_overlapping_sample_weight_3class.jsonl",  
  #"multinli_1.0_train.jsonl",
  "validation_data": "multinli_1.0_dev_matched.jsonl", 
  "test_data": "multinli_1.0_dev_mismatched.jsonl",
  
  # dataloader
  "data_loader": {
    "batch_sampler": {
       # bucket type in allenlp by grouping by length and dynamic padding
      "group_by_length": True, 
      "dynamic_padding": True,
      "batch_size" : 32   #64
    }
  },

  # models; 
  # follow allennlp setups, the rest is the default of bert-base-uncased
  "model": {
    "model_name": "../bert-base-uncased-mnli/",
    transformer_model,
    "max_length": 512,
    "is_load_trained_model": True
  },
  "dropout": 0.1,
  "namespace": "tags",
  
  # Trainer
  "num_epochs": 15,
  # recheck in Trainer 
  "validation_metric": "accuracy",
  "learning_rate_scheduler": {
    "type": "slanted_triangular", # override from allennlp
    "cut_frac": 0.06
  },
  "optimizer": {
    # using optimizer from the transformers package
    "type": "huggingface_adamw", 
    "lr": 2.0e-6 ,  #2.909e-07(ours), #2e-6(PCGU), #5e-5,
    "weight_decay": 0.1,
  },
  "cuda_device" : 0,
  "evaluation_strategy": "steps",
  "eval_steps": 100,
  "load_best_model_at_end": True,
  "save_total_limit": 1,
  # "use_amp" in allennlp
  "half_precision_backend": "amp",
 # ******************** soft masking neurons ********************
  "weaken_rate": null,
  "get_masking_value": False,
  # percent of neurons to mask
  "masking_rate": 0.05, 
  "neuron_group": null,
  "rank_losses": False, 
  # range 0.0 to 0.10, step = 0.01
  # searching masking rates
  "percent": {
    "low": 0.0, 
    "high": 0.1,
    "step": 0.01,
    "size": 50,
    "mode": 0o666,
    "acc": {}
    },
  # range 0 to 1, step = 0.1
  "epsilons": {
    "low":  0, 
    "high": 1,
    "step": 0.1, #0.0001, #0.01, 0.1,
    "size": 50,
    "mode": 0o666,
    "acc": {}
    },
  "single_neuron": False,
  # do hyperparameter search on value used modify neurons's activations
  "soft_masking_value_search": True,
  # do hyperparameter search on the percentages of all neurons to mask
  "masking_rate_search": False,
  "get_condition_inferences": False,
  "get_condition_inference_scores": False,
  # ******************** test  stuff ********************
  "eval_model": True,
  "dev-name": "hans",
  "prunning": False,
  # uppper bound test on test set
  # use to read prediction txt files
  "prediction_path": '../pickles/prediction/',
  "evaluations":  {},
  "to_text": True,
  # "get_result": False,
  "get_condition_hans_result": False,
  "get_hans_result": True,
  "save_rank": True,
  # neurons components
  "attention_components": 3,
  "nums": {},

}
