# Masking as an Efficient Alternative to Finetuning
for Pretrained Language Models

Wl âˆˆ {Wl_K , Wl_Q, Wl_V , Wl_AO, Wl_I , Wl_O}

WP and WT are always masked

they mask top-down and bottom up approach with sparsity %5 quite good in Fig2

ref- https://aclanthology.org/2020.emnlp-main.174.pdf



