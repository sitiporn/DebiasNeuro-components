{'layer': -1, 'treatment': True, 'analysis': False, 'print_config': True, 'topk': False, 'distribution': False, 'embedding_summary': False, 'getting_counterfactual': False, 'traced': False, 'debias_test': False, 'partition_params': True, 'get_prediction': False, 'dev-name': 'matched', 'weaken': 0.97, 'masking_rate': 0.05, 'neuron_group': None, 'diag': False, 'rank_losses': False, 'model_name': '../bert-base-uncased-mnli/', 'is_group_by_class': False, 'is_averaged_embeddings': True, 'intervention_type': 'weaken', 'upper_bound': 95, 'lower_bound': 5, 'seed': 42, 'collect_representation': True, 'DEBUG': True, 'debug': False, 'num_samples': 300, 'label_maps': {'contradiction': 0, 'entailment': 1, 'neutral': 2}, 'layers': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'heads': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'k': None, 'num_top_neurons': None, 'range_percents': True, 'neurons': True, 'dev_path': '../debias_fork_clean/debias_nlu_clean/data/nli/', 'exp_json': 'multinli_1.0_dev_matched.jsonl', 'dev_json': {}, 'counterfactual_paths': [], 'NIE_paths': [], 'is_NIE_exist': [], 'is_counterfactual_exist': [], 'percent': {'low': 0.0, 'high': 0.1, 'step': 0.01, 'size': 50, 'mode': '0o666', 'acc': {}}, 'epsilons': {'low': 0.81, 'high': 0.98, 'step': 0.01, 'size': 50, 'mode': '0o666', 'acc': {}}, 'single_neuron': False, 'eval': {'do': 'High-overlap', 'layer': 1, 'debug': False, 'all_layers': True, 'intervention_mode': 'Intervene'}, 'prediction_path': '../pickles/prediction/', 'evaluations': {}, 'to_text': True, 'get_result': True, 'save_rank': True, 'attention_components': 3, 'nums': {}}
== statistic ==
{'High-overlap': 0.7777777777777778, 'Low-overlap': 0.0}
=========== Configs  ===============
current experiment set :multinli_1.0_dev_matched.jsonl
current dev set: {'matched': 'multinli_1.0_dev_matched.jsonl'}
is_group_by_class : False
is_averaged_embeddings : True
+percent threshold of overlap score
upper_bound : 95
lower_bound : 5
samples used to compute nie scores : 300
Intervention type : weaken
HOL and LOL representation in the following paths 
current: ../pickles/avg_Q_counterfactual_representation.pickle ,  : True 
current: ../pickles/avg_K_counterfactual_representation.pickle ,  : True 
current: ../pickles/avg_V_counterfactual_representation.pickle ,  : True 
current: ../pickles/avg_AO_counterfactual_representation.pickle ,  : True 
current: ../pickles/avg_I_counterfactual_representation.pickle ,  : True 
current: ../pickles/avg_O_counterfactual_representation.pickle ,  : True 
=========== End configs  =========
current datapath : ../debias_fork_clean/debias_nlu_clean/data/nli/
current json files : {'matched': 'multinli_1.0_dev_matched.jsonl'}
bert.embeddings.word_embeddings.weight
bert.embeddings.position_embeddings.weight
bert.embeddings.token_type_embeddings.weight
bert.embeddings.LayerNorm.weight
bert.embeddings.LayerNorm.bias
bert.encoder.layer.0.attention.output.LayerNorm.weight
bert.encoder.layer.0.attention.output.LayerNorm.bias
bert.encoder.layer.0.output.LayerNorm.weight
bert.encoder.layer.0.output.LayerNorm.bias
bert.encoder.layer.1.attention.output.LayerNorm.weight
bert.encoder.layer.1.attention.output.LayerNorm.bias
bert.encoder.layer.1.output.LayerNorm.weight
bert.encoder.layer.1.output.LayerNorm.bias
bert.encoder.layer.2.attention.output.LayerNorm.weight
bert.encoder.layer.2.attention.output.LayerNorm.bias
bert.encoder.layer.2.output.LayerNorm.weight
bert.encoder.layer.2.output.LayerNorm.bias
bert.encoder.layer.3.attention.output.LayerNorm.weight
bert.encoder.layer.3.attention.output.LayerNorm.bias
bert.encoder.layer.3.output.LayerNorm.weight
bert.encoder.layer.3.output.LayerNorm.bias
bert.encoder.layer.4.attention.output.LayerNorm.weight
bert.encoder.layer.4.attention.output.LayerNorm.bias
bert.encoder.layer.4.output.LayerNorm.weight
bert.encoder.layer.4.output.LayerNorm.bias
bert.encoder.layer.5.attention.output.LayerNorm.weight
bert.encoder.layer.5.attention.output.LayerNorm.bias
bert.encoder.layer.5.output.LayerNorm.weight
bert.encoder.layer.5.output.LayerNorm.bias
bert.encoder.layer.6.attention.output.LayerNorm.weight
bert.encoder.layer.6.attention.output.LayerNorm.bias
bert.encoder.layer.6.output.LayerNorm.weight
bert.encoder.layer.6.output.LayerNorm.bias
bert.encoder.layer.7.attention.output.LayerNorm.weight
bert.encoder.layer.7.attention.output.LayerNorm.bias
bert.encoder.layer.7.output.LayerNorm.weight
bert.encoder.layer.7.output.LayerNorm.bias
bert.encoder.layer.8.attention.output.LayerNorm.weight
bert.encoder.layer.8.attention.output.LayerNorm.bias
bert.encoder.layer.8.output.LayerNorm.weight
bert.encoder.layer.8.output.LayerNorm.bias
bert.encoder.layer.9.attention.output.LayerNorm.weight
bert.encoder.layer.9.attention.output.LayerNorm.bias
bert.encoder.layer.9.output.LayerNorm.weight
bert.encoder.layer.9.output.LayerNorm.bias
bert.encoder.layer.10.attention.output.LayerNorm.weight
bert.encoder.layer.10.attention.output.LayerNorm.bias
bert.encoder.layer.10.output.LayerNorm.weight
bert.encoder.layer.10.output.LayerNorm.bias
bert.encoder.layer.11.attention.output.LayerNorm.weight
bert.encoder.layer.11.attention.output.LayerNorm.bias
bert.encoder.layer.11.output.LayerNorm.weight
bert.encoder.layer.11.output.LayerNorm.bias
bert.pooler.dense.weight
bert.pooler.dense.bias
classifier.weight
classifier.bias
saving layer 0's components into pickle files
saving layer 1's components into pickle files
saving layer 2's components into pickle files
saving layer 3's components into pickle files
saving layer 4's components into pickle files
saving layer 5's components into pickle files
saving layer 6's components into pickle files
saving layer 7's components into pickle files
saving layer 8's components into pickle files
saving layer 9's components into pickle files
saving layer 10's components into pickle files
saving layer 11's components into pickle files
