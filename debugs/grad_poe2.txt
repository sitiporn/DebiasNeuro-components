config: ./configs/pcgu_config.yaml
== statistic ==
{'High-overlap': 0.7777777777777778, 'Low-overlap': 0.0}
Counterfactual type: ['High-overlap']
Intervention type : weaken
Loading path for single at seed:409, layer: 11
Q: ../counterfactuals/poe2/seed_409/avg_Q_counterfactual_representation.pickle
K: ../counterfactuals/poe2/seed_409/avg_K_counterfactual_representation.pickle
V: ../counterfactuals/poe2/seed_409/avg_V_counterfactual_representation.pickle
AO: ../counterfactuals/poe2/seed_409/avg_AO_counterfactual_representation.pickle
I: ../counterfactuals/poe2/seed_409/avg_I_counterfactual_representation.pickle
O: ../counterfactuals/poe2/seed_409/avg_O_counterfactual_representation.pickle
NIE_paths: ['../NIE/poe2/seed_409/avg_embeddings_High-overlap_computed_all_layers_.pickle']
Loading model from ../models/debugs/seed_409/checkpoint-1500/pytorch_model.bin
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.8304515866558178
contradiction acc : 0.8447530864197531
entailment acc : 0.8752526710944268
neutral acc : 0.7660594439117929
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9348
subsequence: 0.9924
constituent: 0.975
Heuristic  non-entailed results:
lexical_overlap: 0.2804
subsequence: 0.056
constituent: 0.4488
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.6145666837692261
has score :0.6145666837692261
==================== Avearge scores ===================
average overall acc : 0.8304515866558178
averge contradiction acc : 0.8447530864197531
average entailment acc : 0.8752526710944268
average neutral acc : 0.7660594439117929
avarge hans score : 0.6145666837692261

# NOTE: rerun again of POE and Reweight on 36500
# current is not equal because we dont use the main model (36500)
# using original learning rate -> eval_acc and eval_loss is equal from starting until the end of training (for recent, Poe2, Reweight)
# using huge learning -> the results is greatly declined
# Todo:
# 1. set grad to zero for both: 1) masking_grad 2) reverse_grad on big learning_rate to see how it changes
   - on huge learning
# 2. flip positions of masking grad and reverse_grad

 JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
    47546 gpu-clust    OR_lr sitiporn  R      10:38      1 ist-gpu-07
    47545 gpu-clust    OR_lr sitiporn  R      11:24      1 ist-gpu-10
    47544 gpu-clust    OR_lr sitiporn  R      12:22      1 ist-gpu-08

# Checking thought
1. All frozen required_grad are correct outside of Encoder and LayerNorm
2. Inside Encoder train and freeze numbers of each layer are corrects
3. big lr is broken
4. not changes according to lr


# why below line is no affect?
if param_name == 'bert.encoder.layer.0.attention.self.query.weight':
   model.bert.encoder.layer[0].attention.self.query.weight.register_hook(partial(masking_grad, [*range(0, param.shape[0], 1)] , param_name, DEBUG))
   return model, hooks
        


