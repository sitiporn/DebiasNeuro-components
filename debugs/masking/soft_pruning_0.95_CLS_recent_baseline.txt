config: ./configs/pcgu_config.yaml
== statistic ==
{'High-overlap': 0.7777777777777778, 'Low-overlap': 0.0}
Counterfactual type: ['High-overlap']
Intervention type : weaken
loading NIE : ../NIE/recent_baseline/seed_3990/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_409/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_1548/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_3099/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_3785/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
Loading model from ../models/recent_baseline/seed_3990/checkpoint-36500/pytorch_model.bin
loading top neurons : ../pickles/top_neurons/recent_baseline/top_neuron_3990_percent_High-overlap_all_layers.pickle
freeze whole tensor: bert.embeddings.word_embeddings.weight
freeze whole tensor: bert.embeddings.position_embeddings.weight
freeze whole tensor: bert.embeddings.token_type_embeddings.weight
freeze whole tensor: bert.embeddings.LayerNorm.weight
freeze whole tensor: bert.embeddings.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.bias
freeze whole tensor: bert.pooler.dense.weight
freeze whole tensor: bert.pooler.dense.bias
freeze whole tensor: classifier.weight
freeze whole tensor: classifier.bias
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
count_param : 165888
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
summary after combine parameters
********** weight  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
********** bias  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
saving 0's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer0_collect_param=False_components.pickle
saving 1's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer1_collect_param=False_components.pickle
saving 2's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer2_collect_param=False_components.pickle
saving 3's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer3_collect_param=False_components.pickle
saving 4's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer4_collect_param=False_components.pickle
saving 5's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer5_collect_param=False_components.pickle
saving 6's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer6_collect_param=False_components.pickle
saving 7's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer7_collect_param=False_components.pickle
saving 8's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer8_collect_param=False_components.pickle
saving 9's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer9_collect_param=False_components.pickle
saving 10's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer10_collect_param=False_components.pickle
saving 11's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3990_layer11_collect_param=False_components.pickle
loading NIE : ../NIE/recent_baseline/seed_3990/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving NIE table into ../NIE/recent_baseline/seed_3990/nie_table_avg_embeddings_High-overlap_computed_all_layers_.pickle !
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.0.attention.output.dense.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.0.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0000976657876746, neuron_ids: 3
bert.encoder.layer.0.output.dense.weight, frozen:752, train:16, multiplier: 1.0020876826722338, neuron_ids: 16
bert.encoder.layer.1.attention.self.query.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.1.attention.self.key.weight, frozen:751, train:17, multiplier: 1.0022184523032756, neuron_ids: 17
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.1.attention.output.dense.weight, frozen:758, train:10, multiplier: 1.001303780964798, neuron_ids: 10
bert.encoder.layer.1.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.000065108405495, neuron_ids: 2
bert.encoder.layer.1.output.dense.weight, frozen:758, train:10, multiplier: 1.001303780964798, neuron_ids: 10
bert.encoder.layer.2.attention.self.query.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.2.attention.self.key.weight, frozen:758, train:10, multiplier: 1.001303780964798, neuron_ids: 10
bert.encoder.layer.2.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.2.attention.output.dense.weight, frozen:750, train:18, multiplier: 1.0023492560689113, neuron_ids: 18
bert.encoder.layer.2.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.000065108405495, neuron_ids: 2
bert.encoder.layer.2.output.dense.weight, frozen:737, train:31, multiplier: 1.0040528173617467, neuron_ids: 31
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.3.attention.self.key.weight, frozen:752, train:16, multiplier: 1.0020876826722338, neuron_ids: 16
bert.encoder.layer.3.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.3.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0040528173617467, neuron_ids: 31
bert.encoder.layer.3.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0000325531430059, neuron_ids: 1
bert.encoder.layer.3.output.dense.weight, frozen:711, train:57, multiplier: 1.0074773711137348, neuron_ids: 57
bert.encoder.layer.4.attention.self.query.weight, frozen:752, train:16, multiplier: 1.0020876826722338, neuron_ids: 16
bert.encoder.layer.4.attention.self.key.weight, frozen:761, train:7, multiplier: 1.0009122898475173, neuron_ids: 7
bert.encoder.layer.4.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.4.attention.output.dense.weight, frozen:699, train:69, multiplier: 1.0090658257784786, neuron_ids: 69
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0000976657876746, neuron_ids: 3
bert.encoder.layer.4.output.dense.weight, frozen:683, train:85, multiplier: 1.011191573403555, neuron_ids: 85
bert.encoder.layer.5.attention.self.query.weight, frozen:733, train:35, multiplier: 1.0045781556572924, neuron_ids: 35
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.5.attention.output.dense.weight, frozen:666, train:102, multiplier: 1.0134600158353126, neuron_ids: 102
bert.encoder.layer.5.intermediate.dense.weight, frozen:3063, train:9, multiplier: 1.0002930546058415, neuron_ids: 9
bert.encoder.layer.5.output.dense.weight, frozen:655, train:113, multiplier: 1.0149332628518566, neuron_ids: 113
bert.encoder.layer.6.attention.self.query.weight, frozen:744, train:24, multiplier: 1.0031347962382444, neuron_ids: 24
bert.encoder.layer.6.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.6.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.6.attention.output.dense.weight, frozen:644, train:124, multiplier: 1.0164107993647429, neuron_ids: 124
bert.encoder.layer.6.intermediate.dense.weight, frozen:3054, train:18, multiplier: 1.0005862810240376, neuron_ids: 18
bert.encoder.layer.6.output.dense.weight, frozen:632, train:136, multiplier: 1.0180275715800633, neuron_ids: 136
bert.encoder.layer.7.attention.self.query.weight, frozen:749, train:19, multiplier: 1.0024800939825087, neuron_ids: 19
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.7.attention.self.value.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.7.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.0201912858660993, neuron_ids: 152
bert.encoder.layer.7.intermediate.dense.weight, frozen:3048, train:24, multiplier: 1.0007818608287724, neuron_ids: 24
bert.encoder.layer.7.output.dense.weight, frozen:623, train:145, multiplier: 1.0192435301924347, neuron_ids: 145
bert.encoder.layer.8.attention.self.query.weight, frozen:740, train:28, multiplier: 1.003659174072138, neuron_ids: 28
bert.encoder.layer.8.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.8.attention.output.dense.weight, frozen:597, train:171, multiplier: 1.022772672792648, neuron_ids: 171
bert.encoder.layer.8.intermediate.dense.weight, frozen:3022, train:50, multiplier: 1.0016302575806977, neuron_ids: 50
bert.encoder.layer.8.output.dense.weight, frozen:569, train:199, multiplier: 1.0266007218286317, neuron_ids: 199
bert.encoder.layer.9.attention.self.query.weight, frozen:719, train:49, multiplier: 1.0064211767789282, neuron_ids: 49
bert.encoder.layer.9.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.9.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.9.attention.output.dense.weight, frozen:538, train:230, multiplier: 1.0308724832214755, neuron_ids: 230
bert.encoder.layer.9.intermediate.dense.weight, frozen:3021, train:51, multiplier: 1.001662916951971, neuron_ids: 51
bert.encoder.layer.9.output.dense.weight, frozen:541, train:227, multiplier: 1.0304575338789737, neuron_ids: 227
bert.encoder.layer.10.attention.self.query.weight, frozen:678, train:90, multiplier: 1.0118577075098814, neuron_ids: 90
bert.encoder.layer.10.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.10.attention.self.value.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.10.attention.output.dense.weight, frozen:522, train:246, multiplier: 1.0330912025827268, neuron_ids: 246
bert.encoder.layer.10.intermediate.dense.weight, frozen:3001, train:71, multiplier: 1.0023165519266535, neuron_ids: 71
bert.encoder.layer.10.output.dense.weight, frozen:498, train:270, multiplier: 1.0364372469635614, neuron_ids: 270
bert.encoder.layer.11.attention.self.query.weight, frozen:629, train:139, multiplier: 1.0184325686248503, neuron_ids: 139
bert.encoder.layer.11.attention.self.key.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335, multiplier: 1.0456092579986385, neuron_ids: 335
bert.encoder.layer.11.intermediate.dense.weight, frozen:2911, train:161, multiplier: 1.005268497005792, neuron_ids: 161
bert.encoder.layer.11.output.dense.weight, frozen:309, train:459, multiplier: 1.0635646032405526, neuron_ids: 459
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.8272986167615948
contradiction acc : 0.8564814814814815
entailment acc : 0.8166329771874098
neutral acc : 0.808884627676574
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9576
subsequence: 0.9988
constituent: 0.9982
Heuristic  non-entailed results:
lexical_overlap: 0.491
subsequence: 0.0462
constituent: 0.038
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5882999897003174
has score :0.5882999897003174
Loading model from ../models/recent_baseline/seed_409/checkpoint-36500/pytorch_model.bin
loading top neurons : ../pickles/top_neurons/recent_baseline/top_neuron_409_percent_High-overlap_all_layers.pickle
freeze whole tensor: bert.embeddings.word_embeddings.weight
freeze whole tensor: bert.embeddings.position_embeddings.weight
freeze whole tensor: bert.embeddings.token_type_embeddings.weight
freeze whole tensor: bert.embeddings.LayerNorm.weight
freeze whole tensor: bert.embeddings.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.bias
freeze whole tensor: bert.pooler.dense.weight
freeze whole tensor: bert.pooler.dense.bias
freeze whole tensor: classifier.weight
freeze whole tensor: classifier.bias
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
count_param : 165888
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
summary after combine parameters
********** weight  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
********** bias  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
saving 0's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer0_collect_param=False_components.pickle
saving 1's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer1_collect_param=False_components.pickle
saving 2's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer2_collect_param=False_components.pickle
saving 3's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer3_collect_param=False_components.pickle
saving 4's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer4_collect_param=False_components.pickle
saving 5's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer5_collect_param=False_components.pickle
saving 6's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer6_collect_param=False_components.pickle
saving 7's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer7_collect_param=False_components.pickle
saving 8's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer8_collect_param=False_components.pickle
saving 9's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer9_collect_param=False_components.pickle
saving 10's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer10_collect_param=False_components.pickle
saving 11's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer11_collect_param=False_components.pickle
loading NIE : ../NIE/recent_baseline/seed_409/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving NIE table into ../NIE/recent_baseline/seed_409/nie_table_avg_embeddings_High-overlap_computed_all_layers_.pickle !
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0000325531430059, neuron_ids: 1
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18, multiplier: 1.0023492560689113, neuron_ids: 18
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0000325531430059, neuron_ids: 1
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18, multiplier: 1.0023492560689113, neuron_ids: 18
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0040528173617467, neuron_ids: 31
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0000976657876746, neuron_ids: 3
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44, multiplier: 1.0057621791513882, neuron_ids: 44
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24, multiplier: 1.0031347962382444, neuron_ids: 24
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37, multiplier: 1.0048410310087663, neuron_ids: 37
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0000976657876746, neuron_ids: 3
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61, multiplier: 1.0080063000393753, neuron_ids: 61
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21, multiplier: 1.0027418723070898, neuron_ids: 21
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79, multiplier: 1.010393369293514, neuron_ids: 79
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0000976657876746, neuron_ids: 3
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87, multiplier: 1.0114579217700514, neuron_ids: 87
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29, multiplier: 1.003790354202065, neuron_ids: 29
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113, multiplier: 1.0149332628518566, neuron_ids: 113
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17, multiplier: 1.000553691821646, neuron_ids: 17
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119, multiplier: 1.0157386589075517, neuron_ids: 119
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22, multiplier: 1.002872812744842, neuron_ids: 22
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140, multiplier: 1.018567639257294, neuron_ids: 140
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22, multiplier: 1.000716659065737, neuron_ids: 22
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128, multiplier: 1.0169491525423726, neuron_ids: 128
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14, multiplier: 1.001826245760501, neuron_ids: 14
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144, multiplier: 1.0191082802547766, neuron_ids: 144
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18, multiplier: 1.0005862810240376, neuron_ids: 18
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161, multiplier: 1.021412421864609, neuron_ids: 161
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38, multiplier: 1.0049725202826487, neuron_ids: 38
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169, multiplier: 1.0225003328451598, neuron_ids: 169
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46, multiplier: 1.0014996413901023, neuron_ids: 46
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197, multiplier: 1.0263263397033267, neuron_ids: 197
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47, multiplier: 1.0061574741255077, neuron_ids: 47
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227, multiplier: 1.0304575338789737, neuron_ids: 227
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49, multiplier: 1.0015976003390825, neuron_ids: 49
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243, multiplier: 1.032674465510285, neuron_ids: 243
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105, multiplier: 1.0138613861386137, neuron_ids: 105
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12, multiplier: 1.001564945226917, neuron_ids: 12
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245, multiplier: 1.0329522528581023, neuron_ids: 245
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70, multiplier: 1.0022838499184339, neuron_ids: 70
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273, multiplier: 1.036857027136491, neuron_ids: 273
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122, multiplier: 1.016141836464673, neuron_ids: 122
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8, multiplier: 1.0010427528675703, neuron_ids: 8
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335, multiplier: 1.0456092579986385, neuron_ids: 335
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111, multiplier: 1.0036263843967461, neuron_ids: 111
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421, multiplier: 1.057996969279518, neuron_ids: 421
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.8232302685109846
contradiction acc : 0.858641975308642
entailment acc : 0.8486861103089807
neutral acc : 0.7583892617449665
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.944
subsequence: 0.998
constituent: 0.9928
Heuristic  non-entailed results:
lexical_overlap: 0.489
subsequence: 0.0764
constituent: 0.09
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5983666777610779
has score :0.5983666777610779
Loading model from ../models/recent_baseline/seed_1548/checkpoint-36500/pytorch_model.bin
loading top neurons : ../pickles/top_neurons/recent_baseline/top_neuron_1548_percent_High-overlap_all_layers.pickle
freeze whole tensor: bert.embeddings.word_embeddings.weight
freeze whole tensor: bert.embeddings.position_embeddings.weight
freeze whole tensor: bert.embeddings.token_type_embeddings.weight
freeze whole tensor: bert.embeddings.LayerNorm.weight
freeze whole tensor: bert.embeddings.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.bias
freeze whole tensor: bert.pooler.dense.weight
freeze whole tensor: bert.pooler.dense.bias
freeze whole tensor: classifier.weight
freeze whole tensor: classifier.bias
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
count_param : 165888
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
summary after combine parameters
********** weight  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
********** bias  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
saving 0's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer0_collect_param=False_components.pickle
saving 1's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer1_collect_param=False_components.pickle
saving 2's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer2_collect_param=False_components.pickle
saving 3's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer3_collect_param=False_components.pickle
saving 4's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer4_collect_param=False_components.pickle
saving 5's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer5_collect_param=False_components.pickle
saving 6's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer6_collect_param=False_components.pickle
saving 7's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer7_collect_param=False_components.pickle
saving 8's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer8_collect_param=False_components.pickle
saving 9's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer9_collect_param=False_components.pickle
saving 10's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer10_collect_param=False_components.pickle
saving 11's components into ../pickles/restore_weight/recent_baseline/masking-0.05/1548_layer11_collect_param=False_components.pickle
loading NIE : ../NIE/recent_baseline/seed_1548/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving NIE table into ../NIE/recent_baseline/seed_1548/nie_table_avg_embeddings_High-overlap_computed_all_layers_.pickle !
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.0.attention.output.dense.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.0.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0000976657876746, neuron_ids: 3
bert.encoder.layer.0.output.dense.weight, frozen:756, train:12, multiplier: 1.001564945226917, neuron_ids: 12
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.1.attention.self.key.weight, frozen:747, train:21, multiplier: 1.0027418723070898, neuron_ids: 21
bert.encoder.layer.1.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.1.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0009122898475173, neuron_ids: 7
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0000325531430059, neuron_ids: 1
bert.encoder.layer.1.output.dense.weight, frozen:760, train:8, multiplier: 1.0010427528675703, neuron_ids: 8
bert.encoder.layer.2.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.2.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.2.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.2.attention.output.dense.weight, frozen:749, train:19, multiplier: 1.0024800939825087, neuron_ids: 19
bert.encoder.layer.2.intermediate.dense.weight, frozen:3068, train:4, multiplier: 1.0001302252897513, neuron_ids: 4
bert.encoder.layer.2.output.dense.weight, frozen:732, train:36, multiplier: 1.0047095761381475, neuron_ids: 36
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.3.attention.self.key.weight, frozen:752, train:16, multiplier: 1.0020876826722338, neuron_ids: 16
bert.encoder.layer.3.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.3.attention.output.dense.weight, frozen:743, train:25, multiplier: 1.0032658393207055, neuron_ids: 25
bert.encoder.layer.3.intermediate.dense.weight, frozen:3068, train:4, multiplier: 1.0001302252897513, neuron_ids: 4
bert.encoder.layer.3.output.dense.weight, frozen:710, train:58, multiplier: 1.0076095512988719, neuron_ids: 58
bert.encoder.layer.4.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0016955784531107, neuron_ids: 13
bert.encoder.layer.4.attention.self.key.weight, frozen:759, train:9, multiplier: 1.0011732499022292, neuron_ids: 9
bert.encoder.layer.4.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.4.attention.output.dense.weight, frozen:714, train:54, multiplier: 1.0070810385523212, neuron_ids: 54
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0000976657876746, neuron_ids: 3
bert.encoder.layer.4.output.dense.weight, frozen:695, train:73, multiplier: 1.0095964243459972, neuron_ids: 73
bert.encoder.layer.5.attention.self.query.weight, frozen:742, train:26, multiplier: 1.0033969166448915, neuron_ids: 26
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.5.attention.output.dense.weight, frozen:678, train:90, multiplier: 1.0118577075098814, neuron_ids: 90
bert.encoder.layer.5.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0004233562379914, neuron_ids: 13
bert.encoder.layer.5.output.dense.weight, frozen:656, train:112, multiplier: 1.0147991543340378, neuron_ids: 112
bert.encoder.layer.6.attention.self.query.weight, frozen:754, train:14, multiplier: 1.001826245760501, neuron_ids: 14
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.6.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.6.attention.output.dense.weight, frozen:638, train:130, multiplier: 1.0172185430463574, neuron_ids: 130
bert.encoder.layer.6.intermediate.dense.weight, frozen:3057, train:15, multiplier: 1.0004885197850513, neuron_ids: 15
bert.encoder.layer.6.output.dense.weight, frozen:652, train:116, multiplier: 1.0153358011634053, neuron_ids: 116
bert.encoder.layer.7.attention.self.query.weight, frozen:748, train:20, multiplier: 1.0026109660574412, neuron_ids: 20
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.7.attention.output.dense.weight, frozen:621, train:147, multiplier: 1.0195141377937071, neuron_ids: 147
bert.encoder.layer.7.intermediate.dense.weight, frozen:3057, train:15, multiplier: 1.0004885197850513, neuron_ids: 15
bert.encoder.layer.7.output.dense.weight, frozen:619, train:149, multiplier: 1.0197848891249497, neuron_ids: 149
bert.encoder.layer.8.attention.self.query.weight, frozen:728, train:40, multiplier: 1.0052356020942408, neuron_ids: 40
bert.encoder.layer.8.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.8.attention.output.dense.weight, frozen:582, train:186, multiplier: 1.024819855884707, neuron_ids: 186
bert.encoder.layer.8.intermediate.dense.weight, frozen:3025, train:47, multiplier: 1.001532292243993, neuron_ids: 47
bert.encoder.layer.8.output.dense.weight, frozen:547, train:221, multiplier: 1.0296286365464529, neuron_ids: 221
bert.encoder.layer.9.attention.self.query.weight, frozen:723, train:45, multiplier: 1.005893909626719, neuron_ids: 45
bert.encoder.layer.9.attention.self.key.weight, frozen:753, train:15, multiplier: 1.0019569471624266, neuron_ids: 15
bert.encoder.layer.9.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.9.attention.output.dense.weight, frozen:530, train:238, multiplier: 1.0319806503628046, neuron_ids: 238
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49, multiplier: 1.0015976003390825, neuron_ids: 49
bert.encoder.layer.9.output.dense.weight, frozen:528, train:240, multiplier: 1.0322580645161277, neuron_ids: 240
bert.encoder.layer.10.attention.self.query.weight, frozen:650, train:118, multiplier: 1.0156043374768577, neuron_ids: 118
bert.encoder.layer.10.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.10.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.10.attention.output.dense.weight, frozen:482, train:286, multiplier: 1.038680010819582, neuron_ids: 286
bert.encoder.layer.10.intermediate.dense.weight, frozen:3008, train:64, multiplier: 1.0020876826722338, neuron_ids: 64
bert.encoder.layer.10.output.dense.weight, frozen:457, train:311, multiplier: 1.0422038268421758, neuron_ids: 311
bert.encoder.layer.11.attention.self.query.weight, frozen:632, train:136, multiplier: 1.0180275715800633, neuron_ids: 136
bert.encoder.layer.11.attention.self.key.weight, frozen:759, train:9, multiplier: 1.0011732499022292, neuron_ids: 9
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.11.attention.output.dense.weight, frozen:382, train:386, multiplier: 1.0529202083904596, neuron_ids: 386
bert.encoder.layer.11.intermediate.dense.weight, frozen:3001, train:71, multiplier: 1.0023165519266535, neuron_ids: 71
bert.encoder.layer.11.output.dense.weight, frozen:344, train:424, multiplier: 1.0584343991179743, neuron_ids: 424
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.8284174125305126
contradiction acc : 0.8095679012345679
entailment acc : 0.8775628068149004
neutral acc : 0.793544263342921
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9818
subsequence: 0.9998
constituent: 0.9996
Heuristic  non-entailed results:
lexical_overlap: 0.3318
subsequence: 0.0292
constituent: 0.0048
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5578333139419556
has score :0.5578333139419556
Loading model from ../models/recent_baseline/seed_3099/checkpoint-11000/pytorch_model.bin
loading top neurons : ../pickles/top_neurons/recent_baseline/top_neuron_3099_percent_High-overlap_all_layers.pickle
freeze whole tensor: bert.embeddings.word_embeddings.weight
freeze whole tensor: bert.embeddings.position_embeddings.weight
freeze whole tensor: bert.embeddings.token_type_embeddings.weight
freeze whole tensor: bert.embeddings.LayerNorm.weight
freeze whole tensor: bert.embeddings.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.bias
freeze whole tensor: bert.pooler.dense.weight
freeze whole tensor: bert.pooler.dense.bias
freeze whole tensor: classifier.weight
freeze whole tensor: classifier.bias
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
count_param : 165888
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
summary after combine parameters
********** weight  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
********** bias  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
saving 0's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer0_collect_param=False_components.pickle
saving 1's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer1_collect_param=False_components.pickle
saving 2's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer2_collect_param=False_components.pickle
saving 3's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer3_collect_param=False_components.pickle
saving 4's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer4_collect_param=False_components.pickle
saving 5's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer5_collect_param=False_components.pickle
saving 6's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer6_collect_param=False_components.pickle
saving 7's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer7_collect_param=False_components.pickle
saving 8's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer8_collect_param=False_components.pickle
saving 9's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer9_collect_param=False_components.pickle
saving 10's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer10_collect_param=False_components.pickle
saving 11's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3099_layer11_collect_param=False_components.pickle
loading NIE : ../NIE/recent_baseline/seed_3099/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving NIE table into ../NIE/recent_baseline/seed_3099/nie_table_avg_embeddings_High-overlap_computed_all_layers_.pickle !
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.0.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.0.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.000065108405495, neuron_ids: 2
bert.encoder.layer.0.output.dense.weight, frozen:759, train:9, multiplier: 1.0011732499022292, neuron_ids: 9
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.1.attention.self.key.weight, frozen:761, train:7, multiplier: 1.0009122898475173, neuron_ids: 7
bert.encoder.layer.1.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.1.attention.output.dense.weight, frozen:757, train:11, multiplier: 1.001434346068588, neuron_ids: 11
bert.encoder.layer.1.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.000065108405495, neuron_ids: 2
bert.encoder.layer.1.output.dense.weight, frozen:755, train:13, multiplier: 1.0016955784531107, neuron_ids: 13
bert.encoder.layer.2.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.2.attention.self.key.weight, frozen:760, train:8, multiplier: 1.0010427528675703, neuron_ids: 8
bert.encoder.layer.2.attention.self.value.weight, frozen:761, train:7, multiplier: 1.0009122898475173, neuron_ids: 7
bert.encoder.layer.2.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.001564945226917, neuron_ids: 12
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0000325531430059, neuron_ids: 1
bert.encoder.layer.2.output.dense.weight, frozen:746, train:22, multiplier: 1.002872812744842, neuron_ids: 22
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.3.attention.self.key.weight, frozen:752, train:16, multiplier: 1.0020876826722338, neuron_ids: 16
bert.encoder.layer.3.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.3.attention.output.dense.weight, frozen:757, train:11, multiplier: 1.001434346068588, neuron_ids: 11
bert.encoder.layer.3.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0000325531430059, neuron_ids: 1
bert.encoder.layer.3.output.dense.weight, frozen:736, train:32, multiplier: 1.00418410041841, neuron_ids: 32
bert.encoder.layer.4.attention.self.query.weight, frozen:752, train:16, multiplier: 1.0020876826722338, neuron_ids: 16
bert.encoder.layer.4.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.4.attention.self.value.weight, frozen:761, train:7, multiplier: 1.0009122898475173, neuron_ids: 7
bert.encoder.layer.4.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0040528173617467, neuron_ids: 31
bert.encoder.layer.4.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.000065108405495, neuron_ids: 2
bert.encoder.layer.4.output.dense.weight, frozen:717, train:51, multiplier: 1.006685017695635, neuron_ids: 51
bert.encoder.layer.5.attention.self.query.weight, frozen:736, train:32, multiplier: 1.00418410041841, neuron_ids: 32
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.5.attention.output.dense.weight, frozen:703, train:65, multiplier: 1.0085357846355878, neuron_ids: 65
bert.encoder.layer.5.intermediate.dense.weight, frozen:3058, train:14, multiplier: 1.0004559369504331, neuron_ids: 14
bert.encoder.layer.5.output.dense.weight, frozen:680, train:88, multiplier: 1.0115911485774498, neuron_ids: 88
bert.encoder.layer.6.attention.self.query.weight, frozen:739, train:29, multiplier: 1.003790354202065, neuron_ids: 29
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.6.attention.output.dense.weight, frozen:659, train:109, multiplier: 1.0143970413419625, neuron_ids: 109
bert.encoder.layer.6.intermediate.dense.weight, frozen:3057, train:15, multiplier: 1.0004885197850513, neuron_ids: 15
bert.encoder.layer.6.output.dense.weight, frozen:650, train:118, multiplier: 1.0156043374768577, neuron_ids: 118
bert.encoder.layer.7.attention.self.query.weight, frozen:743, train:25, multiplier: 1.0032658393207055, neuron_ids: 25
bert.encoder.layer.7.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.7.attention.output.dense.weight, frozen:636, train:132, multiplier: 1.0174880763116054, neuron_ids: 132
bert.encoder.layer.7.intermediate.dense.weight, frozen:3055, train:17, multiplier: 1.000553691821646, neuron_ids: 17
bert.encoder.layer.7.output.dense.weight, frozen:592, train:176, multiplier: 1.0234541577825151, neuron_ids: 176
bert.encoder.layer.8.attention.self.query.weight, frozen:727, train:41, multiplier: 1.0053671946589868, neuron_ids: 41
bert.encoder.layer.8.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.8.attention.output.dense.weight, frozen:565, train:203, multiplier: 1.027149926441085, neuron_ids: 203
bert.encoder.layer.8.intermediate.dense.weight, frozen:3037, train:35, multiplier: 1.0011406224539678, neuron_ids: 35
bert.encoder.layer.8.output.dense.weight, frozen:528, train:240, multiplier: 1.0322580645161277, neuron_ids: 240
bert.encoder.layer.9.attention.self.query.weight, frozen:684, train:84, multiplier: 1.0110584518167456, neuron_ids: 84
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.9.attention.output.dense.weight, frozen:495, train:273, multiplier: 1.036857027136491, neuron_ids: 273
bert.encoder.layer.9.intermediate.dense.weight, frozen:2998, train:74, multiplier: 1.0024146707563792, neuron_ids: 74
bert.encoder.layer.9.output.dense.weight, frozen:502, train:266, multiplier: 1.0358780685190168, neuron_ids: 266
bert.encoder.layer.10.attention.self.query.weight, frozen:664, train:104, multiplier: 1.013727560718057, neuron_ids: 104
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.10.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.10.attention.output.dense.weight, frozen:456, train:312, multiplier: 1.0423452768729635, neuron_ids: 312
bert.encoder.layer.10.intermediate.dense.weight, frozen:2990, train:82, multiplier: 1.0026764149095893, neuron_ids: 82
bert.encoder.layer.10.output.dense.weight, frozen:437, train:331, multiplier: 1.0450401415158523, neuron_ids: 331
bert.encoder.layer.11.attention.self.query.weight, frozen:667, train:101, multiplier: 1.0133262963451641, neuron_ids: 101
bert.encoder.layer.11.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.11.attention.self.value.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.11.attention.output.dense.weight, frozen:406, train:362, multiplier: 1.0494670675047837, neuron_ids: 362
bert.encoder.layer.11.intermediate.dense.weight, frozen:2978, train:94, multiplier: 1.0030692875334684, neuron_ids: 94
bert.encoder.layer.11.output.dense.weight, frozen:344, train:424, multiplier: 1.0584343991179743, neuron_ids: 424
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.8264849471114727
contradiction acc : 0.7876543209876543
entailment acc : 0.9090384060063529
neutral acc : 0.7753275806967083
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9938
subsequence: 1.0
constituent: 1.0
Heuristic  non-entailed results:
lexical_overlap: 0.0104
subsequence: 0.0018
constituent: 0.003
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5015000104904175
has score :0.5015000104904175
Loading model from ../models/recent_baseline/seed_3785/checkpoint-36500/pytorch_model.bin
loading top neurons : ../pickles/top_neurons/recent_baseline/top_neuron_3785_percent_High-overlap_all_layers.pickle
freeze whole tensor: bert.embeddings.word_embeddings.weight
freeze whole tensor: bert.embeddings.position_embeddings.weight
freeze whole tensor: bert.embeddings.token_type_embeddings.weight
freeze whole tensor: bert.embeddings.LayerNorm.weight
freeze whole tensor: bert.embeddings.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.bias
freeze whole tensor: bert.pooler.dense.weight
freeze whole tensor: bert.pooler.dense.bias
freeze whole tensor: classifier.weight
freeze whole tensor: classifier.bias
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
count_param : 165888
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
summary after combine parameters
********** weight  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
********** bias  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
saving 0's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer0_collect_param=False_components.pickle
saving 1's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer1_collect_param=False_components.pickle
saving 2's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer2_collect_param=False_components.pickle
saving 3's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer3_collect_param=False_components.pickle
saving 4's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer4_collect_param=False_components.pickle
saving 5's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer5_collect_param=False_components.pickle
saving 6's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer6_collect_param=False_components.pickle
saving 7's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer7_collect_param=False_components.pickle
saving 8's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer8_collect_param=False_components.pickle
saving 9's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer9_collect_param=False_components.pickle
saving 10's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer10_collect_param=False_components.pickle
saving 11's components into ../pickles/restore_weight/recent_baseline/masking-0.05/3785_layer11_collect_param=False_components.pickle
loading NIE : ../NIE/recent_baseline/seed_3785/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving NIE table into ../NIE/recent_baseline/seed_3785/nie_table_avg_embeddings_High-overlap_computed_all_layers_.pickle !
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.0.attention.self.key.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.0.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.0.attention.output.dense.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.0.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.000065108405495, neuron_ids: 2
bert.encoder.layer.0.output.dense.weight, frozen:756, train:12, multiplier: 1.001564945226917, neuron_ids: 12
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.1.attention.self.key.weight, frozen:751, train:17, multiplier: 1.0022184523032756, neuron_ids: 17
bert.encoder.layer.1.attention.self.value.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.1.attention.output.dense.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0000325531430059, neuron_ids: 1
bert.encoder.layer.1.output.dense.weight, frozen:758, train:10, multiplier: 1.001303780964798, neuron_ids: 10
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.2.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.2.attention.output.dense.weight, frozen:750, train:18, multiplier: 1.0023492560689113, neuron_ids: 18
bert.encoder.layer.2.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.000065108405495, neuron_ids: 2
bert.encoder.layer.2.output.dense.weight, frozen:732, train:36, multiplier: 1.0047095761381475, neuron_ids: 36
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.3.attention.self.key.weight, frozen:757, train:11, multiplier: 1.001434346068588, neuron_ids: 11
bert.encoder.layer.3.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.3.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0040528173617467, neuron_ids: 31
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.000065108405495, neuron_ids: 2
bert.encoder.layer.3.output.dense.weight, frozen:713, train:55, multiplier: 1.0072131147540984, neuron_ids: 55
bert.encoder.layer.4.attention.self.query.weight, frozen:739, train:29, multiplier: 1.003790354202065, neuron_ids: 29
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.4.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.4.attention.output.dense.weight, frozen:692, train:76, multiplier: 1.0099947396107312, neuron_ids: 76
bert.encoder.layer.4.intermediate.dense.weight, frozen:3067, train:5, multiplier: 1.0001627869119323, neuron_ids: 5
bert.encoder.layer.4.output.dense.weight, frozen:683, train:85, multiplier: 1.011191573403555, neuron_ids: 85
bert.encoder.layer.5.attention.self.query.weight, frozen:733, train:35, multiplier: 1.0045781556572924, neuron_ids: 35
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.5.attention.output.dense.weight, frozen:667, train:101, multiplier: 1.0133262963451641, neuron_ids: 101
bert.encoder.layer.5.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.000325626831651, neuron_ids: 10
bert.encoder.layer.5.output.dense.weight, frozen:650, train:118, multiplier: 1.0156043374768577, neuron_ids: 118
bert.encoder.layer.6.attention.self.query.weight, frozen:748, train:20, multiplier: 1.0026109660574412, neuron_ids: 20
bert.encoder.layer.6.attention.self.key.weight, frozen:761, train:7, multiplier: 1.0009122898475173, neuron_ids: 7
bert.encoder.layer.6.attention.self.value.weight, frozen:767, train:1, multiplier: 1.0001302252897513, neuron_ids: 1
bert.encoder.layer.6.attention.output.dense.weight, frozen:647, train:121, multiplier: 1.0160074083873525, neuron_ids: 121
bert.encoder.layer.6.intermediate.dense.weight, frozen:3051, train:21, multiplier: 1.0006840613700771, neuron_ids: 21
bert.encoder.layer.6.output.dense.weight, frozen:641, train:127, multiplier: 1.016814510790414, neuron_ids: 127
bert.encoder.layer.7.attention.self.query.weight, frozen:751, train:17, multiplier: 1.0022184523032756, neuron_ids: 17
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0, neuron_ids: 0
bert.encoder.layer.7.attention.output.dense.weight, frozen:634, train:134, multiplier: 1.0177577524516297, neuron_ids: 134
bert.encoder.layer.7.intermediate.dense.weight, frozen:3055, train:17, multiplier: 1.000553691821646, neuron_ids: 17
bert.encoder.layer.7.output.dense.weight, frozen:621, train:147, multiplier: 1.0195141377937071, neuron_ids: 147
bert.encoder.layer.8.attention.self.query.weight, frozen:741, train:27, multiplier: 1.0035280282242258, neuron_ids: 27
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.8.attention.self.value.weight, frozen:765, train:3, multiplier: 1.0003907776475185, neuron_ids: 3
bert.encoder.layer.8.attention.output.dense.weight, frozen:588, train:180, multiplier: 1.0239999999999991, neuron_ids: 180
bert.encoder.layer.8.intermediate.dense.weight, frozen:3024, train:48, multiplier: 1.001564945226917, neuron_ids: 48
bert.encoder.layer.8.output.dense.weight, frozen:556, train:212, multiplier: 1.0283877878950176, neuron_ids: 212
bert.encoder.layer.9.attention.self.query.weight, frozen:732, train:36, multiplier: 1.0047095761381475, neuron_ids: 36
bert.encoder.layer.9.attention.self.key.weight, frozen:757, train:11, multiplier: 1.001434346068588, neuron_ids: 11
bert.encoder.layer.9.attention.self.value.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.9.attention.output.dense.weight, frozen:528, train:240, multiplier: 1.0322580645161277, neuron_ids: 240
bert.encoder.layer.9.intermediate.dense.weight, frozen:3021, train:51, multiplier: 1.001662916951971, neuron_ids: 51
bert.encoder.layer.9.output.dense.weight, frozen:514, train:254, multiplier: 1.034204147589549, neuron_ids: 254
bert.encoder.layer.10.attention.self.query.weight, frozen:655, train:113, multiplier: 1.0149332628518566, neuron_ids: 113
bert.encoder.layer.10.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0006514657980456, neuron_ids: 5
bert.encoder.layer.10.attention.self.value.weight, frozen:764, train:4, multiplier: 1.000521104742053, neuron_ids: 4
bert.encoder.layer.10.attention.output.dense.weight, frozen:544, train:224, multiplier: 1.0300429184549345, neuron_ids: 224
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70, multiplier: 1.0022838499184339, neuron_ids: 70
bert.encoder.layer.10.output.dense.weight, frozen:512, train:256, multiplier: 1.0344827586206882, neuron_ids: 256
bert.encoder.layer.11.attention.self.query.weight, frozen:611, train:157, multiplier: 1.0208693340422699, neuron_ids: 157
bert.encoder.layer.11.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0007818608287724, neuron_ids: 6
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0002604845011722, neuron_ids: 2
bert.encoder.layer.11.attention.output.dense.weight, frozen:472, train:296, multiplier: 1.0400866738894896, neuron_ids: 296
bert.encoder.layer.11.intermediate.dense.weight, frozen:2903, train:169, multiplier: 1.0055317338221335, neuron_ids: 169
bert.encoder.layer.11.output.dense.weight, frozen:297, train:471, multiplier: 1.065334997919272, neuron_ids: 471
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.8302481692432873
contradiction acc : 0.8160493827160494
entailment acc : 0.8634132255269997
neutral acc : 0.8082454458293384
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9912
subsequence: 1.0
constituent: 0.9994
Heuristic  non-entailed results:
lexical_overlap: 0.2666
subsequence: 0.131
constituent: 0.201
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5982000231742859
has score :0.5982000231742859
==================== Avearge scores ===================
average overall acc : 0.8271358828315704
averge contradiction acc : 0.825679012345679
average entailment acc : 0.8630667051689287
average neutral acc : 0.7888782358581017
avarge hans score : 0.5688400268554688
