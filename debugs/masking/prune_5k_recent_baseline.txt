config: ./configs/pcgu_config.yaml
== statistic ==
{'High-overlap': 0.7777777777777778, 'Low-overlap': 0.0}
Counterfactual type: ['High-overlap']
Intervention type : weaken
loading NIE : ../NIE/recent_baseline/seed_3990/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_409/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_1548/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_3099/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_3785/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.05 neurons :+++++++++
Done saving top neurons into pickle !
loading top neurons : ../pickles/top_neurons/recent_baseline/top_neuron_409_percent_High-overlap_all_layers.pickle
freeze whole tensor: bert.embeddings.word_embeddings.weight
freeze whole tensor: bert.embeddings.position_embeddings.weight
freeze whole tensor: bert.embeddings.token_type_embeddings.weight
freeze whole tensor: bert.embeddings.LayerNorm.weight
freeze whole tensor: bert.embeddings.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.bias
freeze whole tensor: bert.pooler.dense.weight
freeze whole tensor: bert.pooler.dense.bias
freeze whole tensor: classifier.weight
freeze whole tensor: classifier.bias
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
count_param : 165888
# weight train parameters:  4147 
# weight freeze parameters: 78797 
# weight total oparameters: 82944 
# bias train parameters:  4147 
# bias freeze parameters: 78797 
# bias total oparameters: 82944 
summary after combine parameters
********** weight  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
********** bias  ************
# Train : 4147
# Freeze : 78797
# Total  : 82944
saving 0's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer0_collect_param=False_components.pickle
saving 1's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer1_collect_param=False_components.pickle
saving 2's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer2_collect_param=False_components.pickle
saving 3's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer3_collect_param=False_components.pickle
saving 4's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer4_collect_param=False_components.pickle
saving 5's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer5_collect_param=False_components.pickle
saving 6's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer6_collect_param=False_components.pickle
saving 7's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer7_collect_param=False_components.pickle
saving 8's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer8_collect_param=False_components.pickle
saving 9's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer9_collect_param=False_components.pickle
saving 10's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer10_collect_param=False_components.pickle
saving 11's components into ../pickles/restore_weight/recent_baseline/masking-0.05/409_layer11_collect_param=False_components.pickle
Loading model from ../models/recent_baseline/seed_3990/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0420624151967435
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44, multiplier: 1.0607734806629834
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24, multiplier: 1.032258064516129
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37, multiplier: 1.0506155950752394
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61, multiplier: 1.0862800565770863
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21, multiplier: 1.0281124497991967
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79, multiplier: 1.1146589259796806
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87, multiplier: 1.1277533039647578
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29, multiplier: 1.0392422192151556
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113, multiplier: 1.1725190839694657
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17, multiplier: 1.0055646481178395
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119, multiplier: 1.1833590138674885
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22, multiplier: 1.029490616621984
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140, multiplier: 1.2229299363057324
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22, multiplier: 1.0072131147540984
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128, multiplier: 1.2
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144, multiplier: 1.2307692307692308
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18, multiplier: 1.005893909626719
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161, multiplier: 1.2652388797364085
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38, multiplier: 1.0520547945205478
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169, multiplier: 1.2821368948247078
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46, multiplier: 1.0152015862524786
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197, multiplier: 1.3450087565674256
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47, multiplier: 1.0651872399445215
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227, multiplier: 1.4195933456561922
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49, multiplier: 1.0162090638438637
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243, multiplier: 1.4628571428571429
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105, multiplier: 1.158371040723982
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245, multiplier: 1.468451242829828
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70, multiplier: 1.0233177881412392
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273, multiplier: 1.5515151515151515
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122, multiplier: 1.1888544891640866
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335, multiplier: 1.7736720554272518
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111, multiplier: 1.0374873353596759
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421, multiplier: 2.213256484149856
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.35414971521562244
contradiction acc : 0.0
entailment acc : 0.9979786312445856
neutral acc : 0.008309364014062001
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 1.0
subsequence: 0.9928
constituent: 0.991
Heuristic  non-entailed results:
lexical_overlap: 0.0
subsequence: 0.0044
constituent: 0.0174
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5009333491325378
has score :0.5009333491325378
Loading model from ../models/recent_baseline/seed_409/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0420624151967435
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44, multiplier: 1.0607734806629834
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24, multiplier: 1.032258064516129
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37, multiplier: 1.0506155950752394
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61, multiplier: 1.0862800565770863
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21, multiplier: 1.0281124497991967
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79, multiplier: 1.1146589259796806
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87, multiplier: 1.1277533039647578
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29, multiplier: 1.0392422192151556
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113, multiplier: 1.1725190839694657
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17, multiplier: 1.0055646481178395
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119, multiplier: 1.1833590138674885
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22, multiplier: 1.029490616621984
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140, multiplier: 1.2229299363057324
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22, multiplier: 1.0072131147540984
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128, multiplier: 1.2
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144, multiplier: 1.2307692307692308
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18, multiplier: 1.005893909626719
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161, multiplier: 1.2652388797364085
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38, multiplier: 1.0520547945205478
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169, multiplier: 1.2821368948247078
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46, multiplier: 1.0152015862524786
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197, multiplier: 1.3450087565674256
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47, multiplier: 1.0651872399445215
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227, multiplier: 1.4195933456561922
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49, multiplier: 1.0162090638438637
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243, multiplier: 1.4628571428571429
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105, multiplier: 1.158371040723982
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245, multiplier: 1.468451242829828
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70, multiplier: 1.0233177881412392
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273, multiplier: 1.5515151515151515
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122, multiplier: 1.1888544891640866
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335, multiplier: 1.7736720554272518
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111, multiplier: 1.0374873353596759
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421, multiplier: 2.213256484149856
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.35130187144019526
contradiction acc : 0.00030864197530864197
entailment acc : 0.9350274328616807
neutral acc : 0.06871204857782039
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9834
subsequence: 0.9604
constituent: 0.9736
Heuristic  non-entailed results:
lexical_overlap: 0.0252
subsequence: 0.0118
constituent: 0.0244
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.49646666646003723
has score :0.49646666646003723
Loading model from ../models/recent_baseline/seed_1548/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0420624151967435
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44, multiplier: 1.0607734806629834
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24, multiplier: 1.032258064516129
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37, multiplier: 1.0506155950752394
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61, multiplier: 1.0862800565770863
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21, multiplier: 1.0281124497991967
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79, multiplier: 1.1146589259796806
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87, multiplier: 1.1277533039647578
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29, multiplier: 1.0392422192151556
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113, multiplier: 1.1725190839694657
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17, multiplier: 1.0055646481178395
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119, multiplier: 1.1833590138674885
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22, multiplier: 1.029490616621984
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140, multiplier: 1.2229299363057324
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22, multiplier: 1.0072131147540984
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128, multiplier: 1.2
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144, multiplier: 1.2307692307692308
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18, multiplier: 1.005893909626719
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161, multiplier: 1.2652388797364085
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38, multiplier: 1.0520547945205478
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169, multiplier: 1.2821368948247078
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46, multiplier: 1.0152015862524786
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197, multiplier: 1.3450087565674256
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47, multiplier: 1.0651872399445215
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227, multiplier: 1.4195933456561922
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49, multiplier: 1.0162090638438637
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243, multiplier: 1.4628571428571429
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105, multiplier: 1.158371040723982
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245, multiplier: 1.468451242829828
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70, multiplier: 1.0233177881412392
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273, multiplier: 1.5515151515151515
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122, multiplier: 1.1888544891640866
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335, multiplier: 1.7736720554272518
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111, multiplier: 1.0374873353596759
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421, multiplier: 2.213256484149856
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.35181041497152155
contradiction acc : 0.06388888888888888
entailment acc : 0.8663008951775917
neutral acc : 0.08053691275167785
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.628
subsequence: 0.5292
constituent: 0.7962
Heuristic  non-entailed results:
lexical_overlap: 0.389
subsequence: 0.3398
constituent: 0.2028
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.48083335161209106
has score :0.48083335161209106
Loading model from ../models/recent_baseline/seed_3099/checkpoint-11000/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0420624151967435
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44, multiplier: 1.0607734806629834
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24, multiplier: 1.032258064516129
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37, multiplier: 1.0506155950752394
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61, multiplier: 1.0862800565770863
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21, multiplier: 1.0281124497991967
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79, multiplier: 1.1146589259796806
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87, multiplier: 1.1277533039647578
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29, multiplier: 1.0392422192151556
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113, multiplier: 1.1725190839694657
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17, multiplier: 1.0055646481178395
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119, multiplier: 1.1833590138674885
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22, multiplier: 1.029490616621984
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140, multiplier: 1.2229299363057324
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22, multiplier: 1.0072131147540984
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128, multiplier: 1.2
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144, multiplier: 1.2307692307692308
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18, multiplier: 1.005893909626719
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161, multiplier: 1.2652388797364085
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38, multiplier: 1.0520547945205478
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169, multiplier: 1.2821368948247078
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46, multiplier: 1.0152015862524786
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197, multiplier: 1.3450087565674256
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47, multiplier: 1.0651872399445215
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227, multiplier: 1.4195933456561922
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49, multiplier: 1.0162090638438637
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243, multiplier: 1.4628571428571429
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105, multiplier: 1.158371040723982
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245, multiplier: 1.468451242829828
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70, multiplier: 1.0233177881412392
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273, multiplier: 1.5515151515151515
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122, multiplier: 1.1888544891640866
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335, multiplier: 1.7736720554272518
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111, multiplier: 1.0374873353596759
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421, multiplier: 2.213256484149856
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.4040886899918633
contradiction acc : 0.06820987654320988
entailment acc : 0.5570314755991914
neutral acc : 0.5826142537551934
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.452
subsequence: 0.3804
constituent: 0.3542
Heuristic  non-entailed results:
lexical_overlap: 0.6396
subsequence: 0.6026
constituent: 0.5904
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5031999349594116
has score :0.5031999349594116
Loading model from ../models/recent_baseline/seed_3785/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18, multiplier: 1.024
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31, multiplier: 1.0420624151967435
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44, multiplier: 1.0607734806629834
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24, multiplier: 1.032258064516129
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37, multiplier: 1.0506155950752394
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61, multiplier: 1.0862800565770863
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21, multiplier: 1.0281124497991967
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6, multiplier: 1.0078740157480315
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79, multiplier: 1.1146589259796806
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87, multiplier: 1.1277533039647578
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29, multiplier: 1.0392422192151556
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113, multiplier: 1.1725190839694657
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17, multiplier: 1.0055646481178395
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119, multiplier: 1.1833590138674885
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22, multiplier: 1.029490616621984
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140, multiplier: 1.2229299363057324
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22, multiplier: 1.0072131147540984
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128, multiplier: 1.2
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144, multiplier: 1.2307692307692308
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18, multiplier: 1.005893909626719
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161, multiplier: 1.2652388797364085
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38, multiplier: 1.0520547945205478
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169, multiplier: 1.2821368948247078
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46, multiplier: 1.0152015862524786
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197, multiplier: 1.3450087565674256
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47, multiplier: 1.0651872399445215
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227, multiplier: 1.4195933456561922
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49, multiplier: 1.0162090638438637
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243, multiplier: 1.4628571428571429
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105, multiplier: 1.158371040723982
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5, multiplier: 1.0065530799475753
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245, multiplier: 1.468451242829828
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70, multiplier: 1.0233177881412392
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273, multiplier: 1.5515151515151515
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122, multiplier: 1.1888544891640866
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335, multiplier: 1.7736720554272518
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111, multiplier: 1.0374873353596759
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421, multiplier: 2.213256484149856
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.3249593165174939
contradiction acc : 0.18302469135802468
entailment acc : 0.3124458561940514
neutral acc : 0.4857782038990093
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.272
subsequence: 0.195
constituent: 0.3342
Heuristic  non-entailed results:
lexical_overlap: 0.8382
subsequence: 0.7404
constituent: 0.6114
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.49853336811065674
has score :0.49853336811065674
==================== Avearge scores ===================
average overall acc : 0.3572620016273393
averge contradiction acc : 0.06308641975308642
average entailment acc : 0.7337568582154202
average neutral acc : 0.2451901565995526
avarge hans score : 0.49599331617355347
