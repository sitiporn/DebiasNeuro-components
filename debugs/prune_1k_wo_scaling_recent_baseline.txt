config: ./configs/pcgu_config.yaml
== statistic ==
{'High-overlap': 0.7777777777777778, 'Low-overlap': 0.0}
Counterfactual type: ['High-overlap']
Intervention type : weaken
loading NIE : ../NIE/recent_baseline/seed_3990/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_409/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_1548/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_3099/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_3785/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading top neurons : ../pickles/top_neurons/recent_baseline/top_neuron_409_percent_High-overlap_all_layers.pickle
freeze whole tensor: bert.embeddings.word_embeddings.weight
freeze whole tensor: bert.embeddings.position_embeddings.weight
freeze whole tensor: bert.embeddings.token_type_embeddings.weight
freeze whole tensor: bert.embeddings.LayerNorm.weight
freeze whole tensor: bert.embeddings.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.bias
freeze whole tensor: bert.pooler.dense.weight
freeze whole tensor: bert.pooler.dense.bias
freeze whole tensor: classifier.weight
freeze whole tensor: classifier.bias
# weight train parameters:  829 
# weight freeze parameters: 82115 
# weight total oparameters: 82944 
# bias train parameters:  829 
# bias freeze parameters: 82115 
# bias total oparameters: 82944 
count_param : 165888
# weight train parameters:  829 
# weight freeze parameters: 82115 
# weight total oparameters: 82944 
# bias train parameters:  829 
# bias freeze parameters: 82115 
# bias total oparameters: 82944 
summary after combine parameters
********** weight  ************
# Train : 829
# Freeze : 82115
# Total  : 82944
********** bias  ************
# Train : 829
# Freeze : 82115
# Total  : 82944
saving 0's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer0_collect_param=False_components.pickle
saving 1's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer1_collect_param=False_components.pickle
saving 2's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer2_collect_param=False_components.pickle
saving 3's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer3_collect_param=False_components.pickle
saving 4's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer4_collect_param=False_components.pickle
saving 5's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer5_collect_param=False_components.pickle
saving 6's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer6_collect_param=False_components.pickle
saving 7's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer7_collect_param=False_components.pickle
saving 8's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer8_collect_param=False_components.pickle
saving 9's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer9_collect_param=False_components.pickle
saving 10's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer10_collect_param=False_components.pickle
saving 11's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer11_collect_param=False_components.pickle
Loading model from ../models/recent_baseline/seed_3990/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.0
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.0
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.0
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.0
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.45433279088689993
contradiction acc : 0.09259259259259259
entailment acc : 0.976609875830205
neutral acc : 0.2508788750399489
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 1.0
subsequence: 0.9998
constituent: 1.0
Heuristic  non-entailed results:
lexical_overlap: 0.0052
subsequence: 0.0
constituent: 0.0
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5008333325386047
has score :0.5008333325386047
Loading model from ../models/recent_baseline/seed_409/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.0
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.0
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.0
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.0
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.3896460537021969
contradiction acc : 0.05555555555555555
entailment acc : 0.9820964481663298
neutral acc : 0.07989773090444231
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 1.0
subsequence: 1.0
constituent: 0.998
Heuristic  non-entailed results:
lexical_overlap: 0.0
subsequence: 0.0
constituent: 0.0016
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.4999333620071411
has score :0.4999333620071411
Loading model from ../models/recent_baseline/seed_1548/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.0
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.0
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.0
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.0
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.4312449145646867
contradiction acc : 0.18641975308641975
entailment acc : 0.9953797285590529
neutral acc : 0.06040268456375839
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 1.0
subsequence: 1.0
constituent: 1.0
Heuristic  non-entailed results:
lexical_overlap: 0.0
subsequence: 0.0
constituent: 0.0
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5
has score :0.5
Loading model from ../models/recent_baseline/seed_3099/checkpoint-11000/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.0
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.0
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.0
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.0
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.3854759967453214
contradiction acc : 0.040740740740740744
entailment acc : 0.9829627490615074
neutral acc : 0.0811760945989134
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 1.0
subsequence: 0.9982
constituent: 0.9984
Heuristic  non-entailed results:
lexical_overlap: 0.0
subsequence: 0.0002
constituent: 0.0
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.4994666278362274
has score :0.4994666278362274
Loading model from ../models/recent_baseline/seed_3785/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.0
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.0
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.0
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.0
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.0
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.0
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.0
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.5172904800650936
contradiction acc : 0.15617283950617283
entailment acc : 0.916546347097892
neutral acc : 0.4493448386065836
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9934
subsequence: 0.9962
constituent: 1.0
Heuristic  non-entailed results:
lexical_overlap: 0.0096
subsequence: 0.0008
constituent: 0.0002
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5000333189964294
has score :0.5000333189964294
==================== Avearge scores ===================
average overall acc : 0.4355980471928397
averge contradiction acc : 0.10629629629629629
average entailment acc : 0.9707190297429975
average neutral acc : 0.1843400447427293
avarge hans score : 0.500053346157074
