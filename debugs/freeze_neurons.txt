config: ./configs/pcgu_config.yaml
== statistic ==
{'High-overlap': 0.7777777777777778, 'Low-overlap': 0.0}
Counterfactual type: ['High-overlap']
Intervention type : weaken
Loading model from ../models/recent_baseline/seed_3990/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.35414971521562244
contradiction acc : 0.0
entailment acc : 0.9979786312445856
neutral acc : 0.008309364014062001
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 1.0
subsequence: 0.9928
constituent: 0.991
Heuristic  non-entailed results:
lexical_overlap: 0.0
subsequence: 0.0044
constituent: 0.0174
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5009333491325378
has score :0.5009333491325378
Loading model from ../models/recent_baseline/seed_409/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.35130187144019526
contradiction acc : 0.00030864197530864197
entailment acc : 0.9350274328616807
neutral acc : 0.06871204857782039
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9834
subsequence: 0.9604
constituent: 0.9736
Heuristic  non-entailed results:
lexical_overlap: 0.0252
subsequence: 0.0118
constituent: 0.0244
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.49646666646003723
has score :0.49646666646003723
Loading model from ../models/recent_baseline/seed_1548/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.35181041497152155
contradiction acc : 0.06388888888888888
entailment acc : 0.8663008951775917
neutral acc : 0.08053691275167785
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.628
subsequence: 0.5292
constituent: 0.7962
Heuristic  non-entailed results:
lexical_overlap: 0.389
subsequence: 0.3398
constituent: 0.2028
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.48083335161209106
has score :0.48083335161209106
Loading model from ../models/recent_baseline/seed_3099/checkpoint-11000/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.4040886899918633
contradiction acc : 0.06820987654320988
entailment acc : 0.5570314755991914
neutral acc : 0.5826142537551934
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.452
subsequence: 0.3804
constituent: 0.3542
Heuristic  non-entailed results:
lexical_overlap: 0.6396
subsequence: 0.6026
constituent: 0.5904
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5031999349594116
has score :0.5031999349594116
Loading model from ../models/recent_baseline/seed_3785/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.0.attention.output.dense.weight, frozen:763, train:5
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.0.output.dense.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.1.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.1.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.1.attention.output.dense.weight, frozen:750, train:18
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1
bert.encoder.layer.1.output.dense.weight, frozen:750, train:18
bert.encoder.layer.2.attention.self.query.weight, frozen:767, train:1
bert.encoder.layer.2.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.2.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.2.attention.output.dense.weight, frozen:737, train:31
bert.encoder.layer.2.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.2.output.dense.weight, frozen:724, train:44
bert.encoder.layer.3.attention.self.query.weight, frozen:766, train:2
bert.encoder.layer.3.attention.self.key.weight, frozen:744, train:24
bert.encoder.layer.3.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.3.attention.output.dense.weight, frozen:731, train:37
bert.encoder.layer.3.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.3.output.dense.weight, frozen:707, train:61
bert.encoder.layer.4.attention.self.query.weight, frozen:747, train:21
bert.encoder.layer.4.attention.self.key.weight, frozen:762, train:6
bert.encoder.layer.4.attention.self.value.weight, frozen:765, train:3
bert.encoder.layer.4.attention.output.dense.weight, frozen:689, train:79
bert.encoder.layer.4.intermediate.dense.weight, frozen:3069, train:3
bert.encoder.layer.4.output.dense.weight, frozen:681, train:87
bert.encoder.layer.5.attention.self.query.weight, frozen:739, train:29
bert.encoder.layer.5.attention.self.key.weight, frozen:767, train:1
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.5.attention.output.dense.weight, frozen:655, train:113
bert.encoder.layer.5.intermediate.dense.weight, frozen:3055, train:17
bert.encoder.layer.5.output.dense.weight, frozen:649, train:119
bert.encoder.layer.6.attention.self.query.weight, frozen:746, train:22
bert.encoder.layer.6.attention.self.key.weight, frozen:763, train:5
bert.encoder.layer.6.attention.self.value.weight, frozen:764, train:4
bert.encoder.layer.6.attention.output.dense.weight, frozen:628, train:140
bert.encoder.layer.6.intermediate.dense.weight, frozen:3050, train:22
bert.encoder.layer.6.output.dense.weight, frozen:640, train:128
bert.encoder.layer.7.attention.self.query.weight, frozen:754, train:14
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.7.attention.output.dense.weight, frozen:624, train:144
bert.encoder.layer.7.intermediate.dense.weight, frozen:3054, train:18
bert.encoder.layer.7.output.dense.weight, frozen:607, train:161
bert.encoder.layer.8.attention.self.query.weight, frozen:730, train:38
bert.encoder.layer.8.attention.self.key.weight, frozen:764, train:4
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0
bert.encoder.layer.8.attention.output.dense.weight, frozen:599, train:169
bert.encoder.layer.8.intermediate.dense.weight, frozen:3026, train:46
bert.encoder.layer.8.output.dense.weight, frozen:571, train:197
bert.encoder.layer.9.attention.self.query.weight, frozen:721, train:47
bert.encoder.layer.9.attention.self.key.weight, frozen:766, train:2
bert.encoder.layer.9.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.9.attention.output.dense.weight, frozen:541, train:227
bert.encoder.layer.9.intermediate.dense.weight, frozen:3023, train:49
bert.encoder.layer.9.output.dense.weight, frozen:525, train:243
bert.encoder.layer.10.attention.self.query.weight, frozen:663, train:105
bert.encoder.layer.10.attention.self.key.weight, frozen:756, train:12
bert.encoder.layer.10.attention.self.value.weight, frozen:763, train:5
bert.encoder.layer.10.attention.output.dense.weight, frozen:523, train:245
bert.encoder.layer.10.intermediate.dense.weight, frozen:3002, train:70
bert.encoder.layer.10.output.dense.weight, frozen:495, train:273
bert.encoder.layer.11.attention.self.query.weight, frozen:646, train:122
bert.encoder.layer.11.attention.self.key.weight, frozen:760, train:8
bert.encoder.layer.11.attention.self.value.weight, frozen:766, train:2
bert.encoder.layer.11.attention.output.dense.weight, frozen:433, train:335
bert.encoder.layer.11.intermediate.dense.weight, frozen:2961, train:111
bert.encoder.layer.11.output.dense.weight, frozen:347, train:421
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.3249593165174939
contradiction acc : 0.18302469135802468
entailment acc : 0.3124458561940514
neutral acc : 0.4857782038990093
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.272
subsequence: 0.195
constituent: 0.3342
Heuristic  non-entailed results:
lexical_overlap: 0.8382
subsequence: 0.7404
constituent: 0.6114
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.49853336811065674
has score :0.49853336811065674
==================== Avearge scores ===================
average overall acc : 0.3572620016273393
averge contradiction acc : 0.06308641975308642
average entailment acc : 0.7337568582154202
average neutral acc : 0.2451901565995526
avarge hans score : 0.49599331617355347
