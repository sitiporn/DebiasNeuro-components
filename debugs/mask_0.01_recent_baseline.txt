config: ./configs/pcgu_config.yaml
== statistic ==
{'High-overlap': 0.7777777777777778, 'Low-overlap': 0.0}
Counterfactual type: ['High-overlap']
Intervention type : weaken
loading NIE : ../NIE/recent_baseline/seed_3990/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_409/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_1548/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_3099/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading NIE : ../NIE/recent_baseline/seed_3785/avg_embeddings_High-overlap_computed_all_layers_.pickle
++++++++ Component-Neuron_id: 0.01 neurons :+++++++++
Done saving top neurons into pickle !
loading top neurons : ../pickles/top_neurons/recent_baseline/top_neuron_409_percent_High-overlap_all_layers.pickle
freeze whole tensor: bert.embeddings.word_embeddings.weight
freeze whole tensor: bert.embeddings.position_embeddings.weight
freeze whole tensor: bert.embeddings.token_type_embeddings.weight
freeze whole tensor: bert.embeddings.LayerNorm.weight
freeze whole tensor: bert.embeddings.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.0.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.1.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.2.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.3.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.4.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.5.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.6.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.7.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.8.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.9.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.10.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.attention.output.LayerNorm.bias
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.weight
freeze whole tensor: bert.encoder.layer.11.output.LayerNorm.bias
freeze whole tensor: bert.pooler.dense.weight
freeze whole tensor: bert.pooler.dense.bias
freeze whole tensor: classifier.weight
freeze whole tensor: classifier.bias
# weight train parameters:  829 
# weight freeze parameters: 82115 
# weight total oparameters: 82944 
# bias train parameters:  829 
# bias freeze parameters: 82115 
# bias total oparameters: 82944 
count_param : 165888
# weight train parameters:  829 
# weight freeze parameters: 82115 
# weight total oparameters: 82944 
# bias train parameters:  829 
# bias freeze parameters: 82115 
# bias total oparameters: 82944 
summary after combine parameters
********** weight  ************
# Train : 829
# Freeze : 82115
# Total  : 82944
********** bias  ************
# Train : 829
# Freeze : 82115
# Total  : 82944
saving 0's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer0_collect_param=False_components.pickle
saving 1's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer1_collect_param=False_components.pickle
saving 2's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer2_collect_param=False_components.pickle
saving 3's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer3_collect_param=False_components.pickle
saving 4's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer4_collect_param=False_components.pickle
saving 5's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer5_collect_param=False_components.pickle
saving 6's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer6_collect_param=False_components.pickle
saving 7's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer7_collect_param=False_components.pickle
saving 8's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer8_collect_param=False_components.pickle
saving 9's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer9_collect_param=False_components.pickle
saving 10's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer10_collect_param=False_components.pickle
saving 11's components into ../pickles/restore_weight/recent_baseline/masking-0.01/409_layer11_collect_param=False_components.pickle
Loading model from ../models/recent_baseline/seed_3990/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0006514657980456
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0267379679144386
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0019569471624266
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0022838499184339
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.1002865329512894
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0042497548218372
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0987124463519313
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.037837837837838
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.2467532467532467
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0032658393207055
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.3862815884476534
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.487693246541904
contradiction acc : 0.11790123456790123
entailment acc : 0.956107421311002
neutral acc : 0.3521891978267817
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9998
subsequence: 0.9998
constituent: 0.9988
Heuristic  non-entailed results:
lexical_overlap: 0.0116
subsequence: 0.0002
constituent: 0.0
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5016999840736389
has score :0.5016999840736389
Loading model from ../models/recent_baseline/seed_409/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0006514657980456
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0267379679144386
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0019569471624266
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0022838499184339
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.1002865329512894
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0042497548218372
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0987124463519313
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.037837837837838
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.2467532467532467
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0032658393207055
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.3862815884476534
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.4024613506916192
contradiction acc : 0.035493827160493825
entailment acc : 0.9462893444989893
neutral acc : 0.18056887184403964
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 1.0
subsequence: 1.0
constituent: 0.9998
Heuristic  non-entailed results:
lexical_overlap: 0.0002
subsequence: 0.0
constituent: 0.0002
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5000333189964294
has score :0.5000333189964294
Loading model from ../models/recent_baseline/seed_1548/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0006514657980456
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0267379679144386
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0019569471624266
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0022838499184339
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.1002865329512894
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0042497548218372
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0987124463519313
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.037837837837838
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.2467532467532467
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0032658393207055
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.3862815884476534
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.47447111472742065
contradiction acc : 0.30833333333333335
entailment acc : 0.9468668784291077
neutral acc : 0.1236816874400767
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 1.0
subsequence: 0.9944
constituent: 0.9922
Heuristic  non-entailed results:
lexical_overlap: 0.0036
subsequence: 0.0082
constituent: 0.0066
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5008333325386047
has score :0.5008333325386047
Loading model from ../models/recent_baseline/seed_3099/checkpoint-11000/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0006514657980456
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0267379679144386
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0019569471624266
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0022838499184339
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.1002865329512894
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0042497548218372
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0987124463519313
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.037837837837838
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.2467532467532467
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0032658393207055
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.3862815884476534
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.43165174938974776
contradiction acc : 0.12345679012345678
entailment acc : 0.9408027721628646
neutral acc : 0.18728028124001278
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.9886
subsequence: 0.8594
constituent: 0.9282
Heuristic  non-entailed results:
lexical_overlap: 0.015
subsequence: 0.0242
constituent: 0.0268
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.47370001673698425
has score :0.47370001673698425
Loading model from ../models/recent_baseline/seed_3785/checkpoint-36500/pytorch_model.bin
bert.encoder.layer.0.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.0.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.0.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.0.output.dense.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.1.attention.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.1.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.1.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.2.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.2.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.2.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.2.output.dense.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.3.attention.self.query.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.3.attention.self.key.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.3.attention.self.value.weight, frozen:766, train:2, multiplier: 1.0026109660574412
bert.encoder.layer.3.attention.output.dense.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.3.intermediate.dense.weight, frozen:3070, train:2, multiplier: 1.0006514657980456
bert.encoder.layer.3.output.dense.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.4.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.4.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.4.attention.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.4.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.4.output.dense.weight, frozen:761, train:7, multiplier: 1.0091984231274638
bert.encoder.layer.5.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.5.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.5.attention.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.5.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.5.output.dense.weight, frozen:760, train:8, multiplier: 1.0105263157894737
bert.encoder.layer.6.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.6.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.6.attention.output.dense.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.6.intermediate.dense.weight, frozen:3071, train:1, multiplier: 1.000325626831651
bert.encoder.layer.6.output.dense.weight, frozen:756, train:12, multiplier: 1.0158730158730158
bert.encoder.layer.7.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.7.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.7.attention.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.7.intermediate.dense.weight, frozen:3069, train:3, multiplier: 1.0009775171065494
bert.encoder.layer.7.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.8.attention.self.query.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.8.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.8.attention.output.dense.weight, frozen:748, train:20, multiplier: 1.0267379679144386
bert.encoder.layer.8.intermediate.dense.weight, frozen:3066, train:6, multiplier: 1.0019569471624266
bert.encoder.layer.8.output.dense.weight, frozen:754, train:14, multiplier: 1.0185676392572944
bert.encoder.layer.9.attention.self.query.weight, frozen:764, train:4, multiplier: 1.0052356020942408
bert.encoder.layer.9.attention.self.key.weight, frozen:767, train:1, multiplier: 1.001303780964798
bert.encoder.layer.9.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.9.attention.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.9.intermediate.dense.weight, frozen:3065, train:7, multiplier: 1.0022838499184339
bert.encoder.layer.9.output.dense.weight, frozen:729, train:39, multiplier: 1.0534979423868314
bert.encoder.layer.10.attention.self.query.weight, frozen:755, train:13, multiplier: 1.0172185430463576
bert.encoder.layer.10.attention.self.key.weight, frozen:765, train:3, multiplier: 1.003921568627451
bert.encoder.layer.10.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.10.attention.output.dense.weight, frozen:698, train:70, multiplier: 1.1002865329512894
bert.encoder.layer.10.intermediate.dense.weight, frozen:3059, train:13, multiplier: 1.0042497548218372
bert.encoder.layer.10.output.dense.weight, frozen:699, train:69, multiplier: 1.0987124463519313
bert.encoder.layer.11.attention.self.query.weight, frozen:740, train:28, multiplier: 1.037837837837838
bert.encoder.layer.11.attention.self.key.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.self.value.weight, frozen:768, train:0, multiplier: 1.0
bert.encoder.layer.11.attention.output.dense.weight, frozen:616, train:152, multiplier: 1.2467532467532467
bert.encoder.layer.11.intermediate.dense.weight, frozen:3062, train:10, multiplier: 1.0032658393207055
bert.encoder.layer.11.output.dense.weight, frozen:554, train:214, multiplier: 1.3862815884476534
eval model using prunning mode
saving without condition distribution into : ../pickles/performances/inference_multinli.pickle
loading distributions and labels from : ../pickles/performances/inference_multinli.pickle
compute_acc modes:['Null']
overall acc : 0.5082384052074858
contradiction acc : 0.175
entailment acc : 0.7868899797863125
neutral acc : 0.5449025247682966
saving without condition distribution into : ../pickles/performances/inference_heuristics.pickle
hans loading from : ../pickles/performances/inference_heuristics.pickle
saving text answer's bert predictions: ../pickles/performances/hans_text_answers.txt
Heuristic  entailed results:
lexical_overlap: 0.8868
subsequence: 0.8088
constituent: 0.977
Heuristic  non-entailed results:
lexical_overlap: 0.1998
subsequence: 0.1626
constituent: 0.0358
saving evaluation predictoins into : ../pickles/performances/hans_scores.txt
average score : 0.5117999911308289
has score :0.5117999911308289
==================== Avearge scores ===================
average overall acc : 0.4609031733116355
averge contradiction acc : 0.15203703703703705
average entailment acc : 0.9153912792376552
average neutral acc : 0.2777245126238415
avarge hans score : 0.4976133406162262
